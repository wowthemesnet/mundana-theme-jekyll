<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Jiwon Kim | Dionne Blog | SpellOnYou</title>

    <!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Jiwon Kim Dionne Blog | SpellOnYou</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Jiwon Kim Dionne Blog" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Iâ€™m afraid only of standing still. I want to remain fresh, thatâ€™s why I blog." />
<meta property="og:description" content="Iâ€™m afraid only of standing still. I want to remain fresh, thatâ€™s why I blog." />
<link rel="canonical" href="http://localhost:4000/" />
<meta property="og:url" content="http://localhost:4000/" />
<meta property="og:site_name" content="SpellOnYou" />
<link rel="next" href="http://localhost:4000/page2" />
<script type="application/ld+json">
{"name":"SpellOnYou","description":"Iâ€™m afraid only of standing still. I want to remain fresh, thatâ€™s why I blog.","@type":"WebSite","url":"http://localhost:4000/","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/assets/images/logo.png"}},"headline":"Jiwon Kim Dionne Blog","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="shortcut icon" type="image/x-icon" href="/assets/images/favicon.ico">

    <!-- Font Awesome Icons -->
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">

    <!-- Google Fonts-->
    <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro&display=swap" rel="stylesheet">
<!-- :400,400i,700 -->
    <!-- Bootstrap Modified -->
    <link rel="stylesheet" href="/assets/css/main.css">

    <!-- Theme Stylesheet -->
    <link rel="stylesheet" href="/assets/css/theme.css">

    <!-- Jquery on header to make sure everything works, the rest  of the scripts in footer for fast loading -->
    <script
    src="https://code.jquery.com/jquery-3.3.1.min.js"
    integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8="
    crossorigin="anonymous"></script>

    <!-- Mathjax Support -->
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<!-- This goes before </head> closing tag, Google Analytics can be placed here --> 


</head>

<body class=" homefirstpage ">

    <!-- Navbar -->
    <nav id="MagicMenu" class="topnav navbar navbar-expand-lg navbar-light bg-white fixed-top">
    <div class="container">
        <a class="navbar-brand" href="/index.html"><strong>SpellOnYou</strong></a>
        <button class="navbar-toggler collapsed" type="button" data-toggle="collapse" data-target="#navbarColor02" aria-controls="navbarColor02" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
        </button>
        <div class="navbar-collapse collapse" id="navbarColor02" style="">
            <ul class="navbar-nav mr-auto d-flex align-items-center">
               <!--  Replace menu links here -->

<li class="nav-item">
<a class="nav-link" href="/index.html">Home</a>
</li>
<li class="nav-item">
<a class="nav-link" href="/authors-list.html">Authors</a>
</li>
<li class="nav-item">
<a class="nav-link" href="/contact.html">Contact</a>
</li>
<li class="nav-item">
<a class="nav-link" href="/categories.html">Categories</a>
</li>
<!-- <li class="nav-item">
<a target="_blank" class="nav-link" href="https://www.wowthemes.net/themes/mundana-ghost/">Ghost</a>
</li> -->
<li class="nav-item">
<a target="_blank" class="nav-link" href="/buy-sal-a-coffee.html">Buy sal a coffee <i class="fa fa-coffee text-danger"></i></a>
<!-- <a target="_blank" class="nav-link" href="https://www.wowthemes.net/donate/">Buy sal a coffee <i class="fa fa-coffee text-danger"></i></a> -->
</li>

            </ul>
            <ul class="navbar-nav ml-auto d-flex align-items-center">
                <script src="/assets/js/lunr.js"></script>

<script>
$(function() {
    $("#lunrsearchresults").on('click', '#btnx', function () {
        $('#lunrsearchresults').hide( 1000 );
        $( "body" ).removeClass( "modal-open" );
    });
});
    

var documents = [{
    "id": 0,
    "url": "http://localhost:4000/404/",
    "title": "",
    "body": " 404 Page not found :(  The requested page could not be found. "
    }, {
    "id": 1,
    "url": "http://localhost:4000/about.html",
    "title": "About",
    "body": "Made with by Sal @wowthemesnet. "
    }, {
    "id": 2,
    "url": "http://localhost:4000/author-dionne.html",
    "title": "dionne",
    "body": "                        dionne Follow:                                                    Posts by dionne:                   		What is a convolution?	: 		  This is the part of Journey which Jeremy recommended us to do. One of the concepts I have to know. 	 			In 				Fast. AI-v3, 								Dec 16, 2019						            		Jeremy Howard	: 		  How he impacted me?	 			In 				People in the World, 								Dec 15, 2019						            		Making my own classifier with my own data	: 		  Making a classifier which can seperate Munich from Berlin and Hamburg!(hoping my well in Munich!ğŸ¤Ÿ)	 			In 				Fast. AI-v3, 								Dec 14, 2019						            		Pytorch	: 		   This is the list when I type	 			In 				library, 								Dec 12, 2019						            		Gradient backward	: 		  â€ Lecture 08 - Deep Learning From Foundations-part2 â€œ	 			In 				Fast. AI-v3, 								Nov 26, 2019						            		ReLU and data init with normailized data	: 		  â€ Lecture 08 - Deep Learning From Foundations-part2 â€œ	 			In 				Fast. AI-v3, 								Nov 23, 2019						            		Broadcasting, Einstein sum, Pytorch operator	: 		  â€ Lecture 08 - Deep Learning From Foundations-part2 â€œ	 			In 				Fast. AI-v3, 								Nov 21, 2019						            		Questions studying Lesson 08	: 		  fast. ai v3 course	 			In 				QnA, 								Nov 20, 2019	            		Julia Evans	: 		  The women who surprised me in many ways. First, she approached me to teaching some concepts drawing cartoons. It was at Hackers news, which was hightest ranks. Personally I have the use . . . 	 			In 				People in the World, 								Nov 20, 2019						            		Why am I not listed as a contributor?!	: 		  From the end of last year, big changes have witnessed in NLP research. Embracing an unprecedented growth, I started to study new exciting results and advances. In doing so, I noticed Iâ€™. . . 	 			In 				Resource, 								Nov 10, 2019						            		Elif Shafak	: 		  For creative-minded people, Istanbul is a treasure. â€™ Photo Â© Chris Boland, licensed under CC BY-NC-ND 2. 0	 			In 				People in the World, 								Nov 05, 2019						            		Retrospective on Pycon 2019 Korea (CoC Committee)	: 		  When I was volunteer, it seems like busy and hectic to managing that crowded conference. In my experience, to get things moving, it needs hierarchy. But it didnâ€™t. Organizers emphasized. . . 	 			In 				Society, 								Nov 05, 2019						            		5 reasons took much time to setting GPU for fast. ai than I expected	: 		  Motivation	 			In 				Resource, 								Oct 23, 2019						            		Lecture 08 - Deep Learning From Foundations-part1	: 		  â€ Lecture 08 - Deep Learning From Foundations-part2 â€œ	 			In 				Fast. AI-v3, 								Oct 18, 2019						            		Lecture 16 - Code-First NLP Note	: 		  Algorithms can encode &amp; magnify human bias	 			In 				Fast. AI-v3, 								Sep 03, 2019						        "
    }, {
    "id": 3,
    "url": "http://localhost:4000/authors-list.html",
    "title": "Authors",
    "body": "Authors:                                                dionne | jiwon kim :       (View Posts)      Welcome, I'm dionne. My old hobby is collecting and systemizing data related with me. (I'm with my kindergarten newsletter) I am an observationist. In the meantime, I enjoy the difference between my expectation and result which I've been facinated(my korean-sarcasm project). And I love a film written and directed by Paul Thomas Anderson. Now, I'm interested in AI ethics, and especially studying bias.                           &nbsp;       &nbsp;                                      "
    }, {
    "id": 4,
    "url": "http://localhost:4000/buy-sal-a-coffee.html",
    "title": "Buy Sal a coffee",
    "body": "Hi! This is Dionne. This page is for Sal, who made the template Iâ€™m using now. He said, 	  I believe in open source philosophy and I think this will be the great future. Why compete when we can cooperate and become better?&nbsp;	Over time I have published quite a good collection of free open source projects now used by thousands of people. And I couldn't be happier. &nbsp;	If for some reason you want to reward my work, I'd feel really appreciated. You would help me tremendously!	and the blog is not actually, I'm not good at design.  &nbsp; And I also belive in open source and itâ€™s a great template, I gave him a token of my gratitude. If you are interested in, you can find further release sal made in here. Buy sal a coffee "
    }, {
    "id": 5,
    "url": "http://localhost:4000/categories.html",
    "title": "Categories",
    "body": "          Categories               Fast. AI-v3:                                  		What is a convolution?	: 		  This is the part of Journey which Jeremy recommended us to do. One of the concepts I have to know. 	 			In 				Fast. AI-v3, 								Dec 16, 2019						                                 		Making my own classifier with my own data	: 		  Making a classifier which can seperate Munich from Berlin and Hamburg!(hoping my well in Munich!ğŸ¤Ÿ)	 			In 				Fast. AI-v3, 								Dec 14, 2019						                                 		Gradient backward	: 		  â€ Lecture 08 - Deep Learning From Foundations-part2 â€œ	 			In 				Fast. AI-v3, 								Nov 26, 2019						                                 		ReLU and data init with normailized data	: 		  â€ Lecture 08 - Deep Learning From Foundations-part2 â€œ	 			In 				Fast. AI-v3, 								Nov 23, 2019						                                 		Broadcasting, Einstein sum, Pytorch operator	: 		  â€ Lecture 08 - Deep Learning From Foundations-part2 â€œ	 			In 				Fast. AI-v3, 								Nov 21, 2019						                                 		Lecture 08 - Deep Learning From Foundations-part1	: 		  â€ Lecture 08 - Deep Learning From Foundations-part2 â€œ	 			In 				Fast. AI-v3, 								Oct 18, 2019						                                 		Lecture 16 - Code-First NLP Note	: 		  Algorithms can encode &amp; magnify human bias	 			In 				Fast. AI-v3, 								Sep 03, 2019						                              Resource:                                  		Why am I not listed as a contributor?!	: 		  From the end of last year, big changes have witnessed in NLP research. Embracing an unprecedented growth, I started to study new exciting results and advances. In doing so, I noticed Iâ€™. . . 	 			In 				Resource, 								Nov 10, 2019						                                 		5 reasons took much time to setting GPU for fast. ai than I expected	: 		  Motivation	 			In 				Resource, 								Oct 23, 2019						                              Society:                                  		Retrospective on Pycon 2019 Korea (CoC Committee)	: 		  When I was volunteer, it seems like busy and hectic to managing that crowded conference. In my experience, to get things moving, it needs hierarchy. But it didnâ€™t. Organizers emphasized. . . 	 			In 				Society, 								Nov 05, 2019						                              People in the World:                                  		Jeremy Howard	: 		  How he impacted me?	 			In 				People in the World, 								Dec 15, 2019						                                 		Julia Evans	: 		  The women who surprised me in many ways. First, she approached me to teaching some concepts drawing cartoons. It was at Hackers news, which was hightest ranks. Personally I have the use . . . 	 			In 				People in the World, 								Nov 20, 2019						                                 		Elif Shafak	: 		  For creative-minded people, Istanbul is a treasure. â€™ Photo Â© Chris Boland, licensed under CC BY-NC-ND 2. 0	 			In 				People in the World, 								Nov 05, 2019						                              QnA:                                  		Questions studying Lesson 08	: 		  fast. ai v3 course	 			In 				QnA, 								Nov 20, 2019	                              library:                                  		Pytorch	: 		   This is the list when I type	 			In 				library, 								Dec 12, 2019						                                             Featured:    				                                          Jeremy Howard                          In                     People in the World,                                                                                           Making my own classifier with my own data                          In                     Fast. AI-v3,                                                                                           Pytorch                          In                     library,                                                                                           Gradient backward                          In                     Fast. AI-v3,                                                                                           ReLU and data init with normailized data                          In                     Fast. AI-v3,                                                                                           Broadcasting, Einstein sum, Pytorch operator                          In                     Fast. AI-v3,                                                                                           Julia Evans                          In                     People in the World,                                                                                           Why am I not listed as a contributor?!                          In                     Resource,                                                                                           5 reasons took much time to setting GPU for fast. ai than I expected                          In                     Resource,                                                                                           Lecture 08 - Deep Learning From Foundations-part1                          In                     Fast. AI-v3,                                                                   "
    }, {
    "id": 6,
    "url": "http://localhost:4000/contact.html",
    "title": "Contact",
    "body": "  Please send your message to SpellOnYou. I will reply as soon as possible!   "
    }, {
    "id": 7,
    "url": "http://localhost:4000/",
    "title": "Jiwon Kim | Dionne Blog",
    "body": "                                  What is a convolution?  :       This is the part of Journey which Jeremy recommended us to do. One of the concepts I have to know.               In                 Fast. AI-v3,                                        Dec 16, 2019                                                                                                                             Jeremy Howard          :                       In                         People in the World,                                                                  Dec 15, 2019                                                                                                                                     Making my own classifier with my own data          :                       In                         Fast. AI-v3,                                                                  Dec 14, 2019                                                                                                                                    Pytorch          :                       In                         library,                                                                  Dec 12, 2019                                                       What is a convolution?                  This is the part of Journey which Jeremy recommended us to do. One of the concepts I have to know.                 Read More            	                                      All Stories:                   		What is a convolution?	: 		  This is the part of Journey which Jeremy recommended us to do. One of the concepts I have to know. 	 			In 				Fast. AI-v3, 								Dec 16, 2019						                  		Jeremy Howard	: 		  How he impacted me?	 			In 				People in the World, 								Dec 15, 2019						                  		Making my own classifier with my own data	: 		  Making a classifier which can seperate Munich from Berlin and Hamburg!(hoping my well in Munich!ğŸ¤Ÿ)	 			In 				Fast. AI-v3, 								Dec 14, 2019						                  		Pytorch	: 		   This is the list when I type	 			In 				library, 								Dec 12, 2019						                  		Gradient backward	: 		  â€ Lecture 08 - Deep Learning From Foundations-part2 â€œ	 			In 				Fast. AI-v3, 								Nov 26, 2019						                  		ReLU and data init with normailized data	: 		  â€ Lecture 08 - Deep Learning From Foundations-part2 â€œ	 			In 				Fast. AI-v3, 								Nov 23, 2019						                  		Broadcasting, Einstein sum, Pytorch operator	: 		  â€ Lecture 08 - Deep Learning From Foundations-part2 â€œ	 			In 				Fast. AI-v3, 								Nov 21, 2019						                  		Questions studying Lesson 08	: 		  fast. ai v3 course	 			In 				QnA, 								Nov 20, 2019	                  		Julia Evans	: 		  The women who surprised me in many ways. First, she approached me to teaching some concepts drawing cartoons. It was at Hackers news, which was hightest ranks. Personally I have the use . . . 	 			In 				People in the World, 								Nov 20, 2019						                  		Why am I not listed as a contributor?!	: 		  From the end of last year, big changes have witnessed in NLP research. Embracing an unprecedented growth, I started to study new exciting results and advances. In doing so, I noticed Iâ€™. . . 	 			In 				Resource, 								Nov 10, 2019						                                                &laquo;                              1                               2                              Next &raquo;                                          Featured:    				                                          Jeremy Howard                          In                     People in the World,                                                                                           Making my own classifier with my own data                          In                     Fast. AI-v3,                                                                                           Pytorch                          In                     library,                                                                                           Gradient backward                          In                     Fast. AI-v3,                                                                                           ReLU and data init with normailized data                          In                     Fast. AI-v3,                                                                                           Broadcasting, Einstein sum, Pytorch operator                          In                     Fast. AI-v3,                                                                                           Julia Evans                          In                     People in the World,                                                                                           Why am I not listed as a contributor?!                          In                     Resource,                                                                                           5 reasons took much time to setting GPU for fast. ai than I expected                          In                     Resource,                                                                                           Lecture 08 - Deep Learning From Foundations-part1                          In                     Fast. AI-v3,                                                               "
    }, {
    "id": 8,
    "url": "http://localhost:4000/privacy-policy.html",
    "title": "Privacy Policy",
    "body": "â€{{site. name}}â€ takes your privacy seriously. To better protect your privacy we provide this privacy policy notice explaining the way your personal information is collected and used. Collection of Routine Information: This website track basic information about their visitors. This information includes, but is not limited to, IP addresses, browser details, timestamps and referring pages. None of this information can personally identify specific visitor to this website. The information is tracked for routine administration and maintenance purposes. Cookies: Where necessary, this website uses cookies to store information about a visitorâ€™s preferences and history in order to better serve the visitor and/or present the visitor with customized content. Advertisement and Other Third Parties: Advertising partners and other third parties may use cookies, scripts and/or web beacons to track visitor activities on this website in order to display advertisements and other useful information. Such tracking is done directly by the third parties through their own servers and is subject to their own privacy policies. This website has no access or control over these cookies, scripts and/or web beacons that may be used by third parties. Learn how to opt out of Googleâ€™s cookie usage. Links to Third Party Websites: We have included links on this website for your use and reference. We are not responsible for the privacy policies on these websites. You should be aware that the privacy policies of these websites may differ from our own. Security: The security of your personal information is important to us, but remember that no method of transmission over the Internet, or method of electronic storage, is 100% secure. While we strive to use commercially acceptable means to protect your personal information, we cannot guarantee its absolute security. Changes To This Privacy Policy: This Privacy Policy is effective and will remain in effect except with respect to any changes in its provisions in the future, which will be in effect immediately after being posted on this page. We reserve the right to update or change our Privacy Policy at any time and you should check this Privacy Policy periodically. If we make any material changes to this Privacy Policy, we will notify you either through the email address you have provided us, or by placing a prominent notice on our website. Contact Information: For any questions or concerns regarding the privacy policy, please contact us here. "
    }, {
    "id": 9,
    "url": "http://localhost:4000/tags.html",
    "title": "Tags",
    "body": "          Tags          {% for tag in site. tags %}     {{ tag[0] }}:           {% assign pages_list = tag[1] %}    {% for post in pages_list %}    {% if post. title != null %}     {% if group == null or group == post. group %}           {% include main-loop-card. html %}     {% endif %}    {% endif %}    {% endfor %}    {% assign pages_list = nil %}    {% assign group = nil %}    {% endfor %}                  {% include sidebar-featured. html %}          "
    }, {
    "id": 10,
    "url": "http://localhost:4000/robots.txt",
    "title": "",
    "body": "      Sitemap: {{ â€œsitemap. xmlâ€   absolute_url }}   "
    }, {
    "id": 11,
    "url": "http://localhost:4000/page2/",
    "title": "Jiwon Kim | Dionne Blog",
    "body": "  {% if page. url ==  /  %}            {% assign latest_post = site. posts[0] %}          &lt;div class= topfirstimage  style= background-image: url({% if latest_post. image contains  ://  %}{{ latest_post. image }}{% else %} {{site. baseurl}}/{{ latest_post. image}}{% endif %}); height: 200px;  background-size: cover;  background-repeat: no-repeat; &gt;&lt;/div&gt;           {{ latest_post. title }}  :       {{ latest_post. excerpt | strip_html | strip_newlines | truncate: 136 }}               In         {% for category in latest_post. categories %}        {{ category }},         {% endfor %}                                {{ latest_post. date | date: '%b %d, %Y' }}                            {%- assign second_post = site. posts[1] -%}                        {% if second_post. image %}                         &lt;img class= w-100  src= {% if second_post. image contains  ://  %}{{ second_post. image }}{% else %}{{ second_post. image | absolute_url }}{% endif %}  alt= {{ second_post. title }} &gt;                        {% endif %}                                    {{ second_post. title }}          :                       In             {% for category in second_post. categories %}            {{ category }},             {% endfor %}                                                      {{ second_post. date | date: '%b %d, %Y' }}                                    {%- assign third_post = site. posts[2] -%}                        {% if third_post. image %}                         &lt;img class= w-100  src= {% if third_post. image contains  ://  %}{{ third_post. image }}{% else %}{{site. baseurl}}/{{ third_post. image }}{% endif %}  alt= {{ third_post. title }} &gt;                        {% endif %}                                    {{ third_post. title }}          :                       In             {% for category in third_post. categories %}            {{ category }},             {% endfor %}                                                      {{ third_post. date | date: '%b %d, %Y' }}                                    {%- assign fourth_post = site. posts[3] -%}                        {% if fourth_post. image %}                        &lt;img class= w-100  src= {% if fourth_post. image contains  ://  %}{{ fourth_post. image }}{% else %}{{site. baseurl}}/{{ fourth_post. image }}{% endif %}  alt= {{ fourth_post. title }} &gt;                        {% endif %}                                    {{ fourth_post. title }}          :                       In             {% for category in fourth_post. categories %}            {{ category }},             {% endfor %}                                                      {{ fourth_post. date | date: '%b %d, %Y' }}                                  {% for post in site. posts %} {% if post. tags contains  sticky  %}                    {{post. title}}                  {{ post. excerpt | strip_html | strip_newlines | truncate: 136 }}                 Read More            	             {% endif %}{% endfor %}  {% endif %}                All Stories:         {% for post in paginator. posts %}          {% include main-loop-card. html %}        {% endfor %}                   {% if paginator. total_pages &gt; 1 %}              {% if paginator. previous_page %}        &laquo; Prev       {% else %}        &laquo;       {% endif %}       {% for page in (1. . paginator. total_pages) %}        {% if page == paginator. page %}        {{ page }}        {% elsif page == 1 %}        {{ page }}        {% else %}        {{ page }}        {% endif %}       {% endfor %}       {% if paginator. next_page %}        Next &raquo;       {% else %}        &raquo;       {% endif %}            {% endif %}                     {% include sidebar-featured. html %}      "
    }, {
    "id": 12,
    "url": "http://localhost:4000/2019/12/convolution/",
    "title": "What is a convolution?",
    "body": "2019/12/16 - This is the part of Journey which Jeremy recommended us to do. One of the concepts I have to know. 1) Kaiming Initializtion in Pytorch was in trouble. 12) Jeremy started to dig in, in lesson09, but I didnâ€™t know why the size of tensor is 2 and even understand this spreadsheet data. 3 Homework: Read Visualizing and Understanding Convolutional Networks paper  What is convolution?     Visualization         one kernel     Matthew D Zeiler &amp; Rob Fergus Paper          Convolution can be represented as matmul   Padding   Kernel has rank 3   How can we find a side-edge, a gradient and area of constant weight?   What is convolution?: A convolutional neural network is that your red, green, and blue pixels go into the simple computation, and something comes out of that, and then the result of that goes into a second layer, and the result of that goes into the third layer and so forth. Visualization: one kernel Refer this site for visualizing CNN filteringMatthew D Zeiler &amp; Rob Fergus PaperLecture01 {align-items: center;}  nine examples of the actual coefficients from the first layer. Convolution can be represented as matmul: CNNs from different viewpoints {align-items: center;}    [A B C D E F G H I J] is 3 by 3 image data flatten to vector.   As a result, convolution is a just matrix just two things happens     Some of entries are set to zeros at all the times   same color always have the same weight. That called weight time / wegith sharing    So, we can implement a convolution with matrix multiplication. But, we donâ€™t do that because itâ€™s slow!Padding:  What most of libraries do is just put zeros asdie of matrix  fast. ai uses reflection paddings (what is this? Jeremy said he uttered it)Kernel has rank 3:  As standard picture input would be 4 5, it would be actually 3d, not 2d.  If we make kernel as a 3x3 size, we pass over same kernel all the different Red, Green, Blue Pixels.      This could make problem, because, if we want to detect frog, which is green, we would want more activations on the green(I made a test cell in my colab 6)   How can we find a side-edge, a gradient and area of constant weight?: Not top-edge!  One kernel can find only the top-edge, so we should stack the kernels 7 So, we pass it through bunch of kernels to the input images, and that process gives us height x width x corresponding number of kernels.  Usually that number of chanel is 16 And if we want to get the more channels and features, we should repeat that process     This process gives rise to memory out of control, we do the stride   #### conv-example. xlsx  2 convolutional filters At a second layer, filter is 3x3x2 tensor, because to add up together the first layerâ€™s channel. Reference:       Problem was math. sqrt(5) was not kaiming initialization formula, Implementation in PytorchÂ &#8617;        size of tensor, lecture09Â &#8617;        conv-example. xlsxÂ &#8617;        Why do computer use red, green and blue instead of primary colorsÂ &#8617;        Grayscale is a group of shades without any visible color. â€¦ Each of these dots has its own brightness level as well and, therefore, can be converted to grayscale. A grayscale image is one with all color information removed. Â &#8617;        Testing RGB and grayscaleÂ &#8617;        stack kernel and make new rank of tensor at output, Lesson06-2019Â &#8617;    "
    }, {
    "id": 13,
    "url": "http://localhost:4000/2019/12/jeremy-howard/",
    "title": "Jeremy Howard",
    "body": "2019/12/15 - How he impacted me?    The person who made me start Computer Vision again. He said, â€œItâ€™s not just something I can throw away. NLP and computer vision a few weeks apart and thatâ€™s going to force your brain to realize like â€˜oh I have to remember thisâ€™â€     â€œKeep going.  Youâ€™re not expected to remember everything. Yet.  Youâ€™re not expected to understand everything. Yet. Youâ€™re not expected to know why everything works.  Yet. â€   His articles are numerous, below.      What is torch. nn Really?   High Performance Numeric Programming with Swift: Explorations and Reflections   C++11, random distributions, and Swift    And especially, I like this book. Designing great data products Great predictive modeling is an important part of the solution, but it no longer stands on its own; as products become more sophisticated, it disappears into the plumbing. Designing great data products And he is also famous for words. Here are some.  weâ€™re going to try and use that to really understand whatâ€™s going on. So to warn you, none of it is rocket science but a lot of its going to look really new. So donâ€™t expect to get it the first time but expect to listen and jump into the notebook try a few things test things out look particularly at like tensor shapes and inputs and outputs to check your understanding then go back and listen again. But and kind of try it, a few times, because you will get there right, itâ€™s just that thereâ€™s going to be a lot of new concepts because we havenâ€™t done that much stuff in pure Pytorch. Lesson 6: Deep Learning 2019 "
    }, {
    "id": 14,
    "url": "http://localhost:4000/2019/12/note02-fastai-1/",
    "title": "Making my own classifier with my own data",
    "body": "2019/12/14 - Making a classifier which can seperate Munich from Berlin and Hamburg!(hoping my well in Munich!ğŸ¤Ÿ) CONTENTS:  1. Creating dataset from google images     1. 1 Get a list of URLs &amp; Download imgs   1. 2 Create ImageDataBunch    2. Train model 3. Interpretation 4. Cleaning Up 5. Putting your model in production 6. Things that can go wrong     6. 1 Learning rate (LR) too high   6. 2 Learning rate (LR) too low   6. 3 Too few epochs   6. 4 Too many epochs   1. Creating dataset from google images: 1. 1 Get a list of URLs &amp; Download imgs:  Go to the google image, type â€œimageâ€ in advanced search,     In my case, I searched â€œMunchâ€ -Berlin -Gutenberg, â€œBerlinâ€ -Munch -Gutenberg, â€œGutenbergâ€ -Munch -Berlin   Then, go to the Advanced search and select type of image: photo   Scroll down when you can see button â€œshow more resultsâ€    Download img urls     opt + cmd + J if you are using mac os, or ctl + shift + j if your os is windows   And type below js code   urls=Array. from(document. querySelectorAll('. rg_i')). map(el=&gt; el. hasAttribute('data-src')?el. getAttribute('data-src'):el. getAttribute('data-iurl'));window. open('data:text/csv;charset=utf-8,' + escape(urls. join('\n')));1. 2 Create ImageDataBunch:  2. Train model: 3. Interpretation: 4. Cleaning Up: 5. Putting your model in production: 6. Things that can go wrong: 6. 1 Learning rate (LR) too high: 6. 2 Learning rate (LR) too low: 6. 3 Too few epochs: 6. 4 Too many epochs: Reference: How to create a deep learning dataset using Google Images "
    }, {
    "id": 15,
    "url": "http://localhost:4000/2019/12/pytorch-tensor/",
    "title": "Pytorch",
    "body": "2019/12/12 -  This is the list when I typedir(torch. tensor)which means these are all tensor methods. â€˜clamp_maxâ€™:â€˜clamp_max_â€™:â€˜clamp_minâ€™:â€˜clamp_min_â€™:check clamp â€˜cloneâ€™:â€˜backwardâ€™:â€˜squeezeâ€™:â€˜squeeze_â€™:â€˜unsqueezeâ€™:â€˜unsqueeze_â€™:â€˜tâ€™:â€˜t_â€™:â€˜gradâ€™:â€˜powâ€™:â€˜pow_â€™:â€˜shapeâ€™:â€˜reluâ€™:â€˜relu_â€™:â€˜sqrtâ€™:â€˜sqrt_â€™:â€˜strideâ€™: â€˜Tâ€™:â€˜absâ€™:â€˜addâ€™:â€˜andâ€™:â€˜arrayâ€™:â€˜array_priorityâ€™:â€˜array_wrapâ€™:â€˜boolâ€™:â€˜classâ€™:â€˜containsâ€™:â€˜deepcopyâ€™:â€˜delattrâ€™:â€˜delitemâ€™:â€˜dictâ€™:â€˜dirâ€™:â€˜divâ€™:â€˜docâ€™:â€˜eqâ€™:â€˜floatâ€™:â€˜floordivâ€™:â€˜formatâ€™:â€˜geâ€™:â€˜getattributeâ€™:â€˜getitemâ€™:â€˜gtâ€™:â€˜hashâ€™:â€˜iaddâ€™:â€˜iandâ€™:â€˜idivâ€™:â€˜ilshiftâ€™:â€˜imulâ€™:â€˜indexâ€™:â€˜initâ€™:â€˜init_subclassâ€™:â€˜intâ€™:â€˜invertâ€™:â€˜iorâ€™:â€˜ipowâ€™:â€˜irshiftâ€™:â€˜isubâ€™:â€˜iterâ€™:â€˜itruedivâ€™:â€˜ixorâ€™:â€˜leâ€™:â€˜lenâ€™:â€˜longâ€™:â€˜lshiftâ€™:â€˜ltâ€™:â€˜matmulâ€™:â€˜modâ€™:â€˜moduleâ€™:â€˜mulâ€™:â€˜neâ€™:â€˜negâ€™:â€˜newâ€™:â€˜nonzeroâ€™:â€˜orâ€™:â€˜powâ€™:â€˜raddâ€™:â€˜rdivâ€™:â€˜reduceâ€™:â€˜reduce_exâ€™:â€˜reprâ€™:â€˜reversedâ€™:â€˜rfloordivâ€™:â€˜rmulâ€™:â€˜rpowâ€™:â€˜rshiftâ€™:â€˜rsubâ€™:â€˜rtruedivâ€™:â€˜setattrâ€™:â€˜setitemâ€™:â€˜setstateâ€™:â€˜sizeofâ€™:â€˜strâ€™:â€˜subâ€™:â€˜subclasshookâ€™:â€˜truedivâ€™:â€˜weakrefâ€™:â€˜xorâ€™:â€˜backward_hooksâ€™:â€˜_baseâ€™:â€˜_cdataâ€™:â€˜_coalescedâ€™:â€˜dimIâ€™:â€˜_dimVâ€™:â€˜_gradâ€™:â€˜_grad_fnâ€™:â€˜_indicesâ€™:â€˜_is_viewâ€™:â€˜_make_subclassâ€™:â€˜_nnzâ€™:â€˜_update_namesâ€™:â€˜_valuesâ€™:â€˜_versionâ€™:â€˜absâ€™:â€˜absâ€™:â€˜acosâ€™:â€˜acos_â€™:â€˜addâ€™:â€˜add_â€™:â€˜addbmmâ€™:â€˜addbmm_â€™:â€˜addcdivâ€™:â€˜addcdiv_â€™:â€˜addcmulâ€™:â€˜addcmul_â€™:â€˜addmmâ€™:â€˜addmm_â€™:â€˜addmvâ€™:â€˜addmv_â€™:â€˜addrâ€™:â€˜addr_â€™:â€˜align_asâ€™:â€˜align_toâ€™:â€˜allâ€™:â€˜allcloseâ€™:â€˜anyâ€™:â€˜apply_â€™:â€˜argmaxâ€™:â€˜argminâ€™:â€˜argsortâ€™:â€˜as_stridedâ€™:â€˜as_strided_â€™:â€˜asinâ€™:â€˜asin_â€™:â€˜atanâ€™:â€˜atan2â€™:â€˜atan2â€™:â€˜atanâ€™:â€˜backwardâ€™:â€˜baddbmmâ€™:â€˜baddbmm_â€™:â€˜bernoulliâ€™:â€˜bernoulli_â€™:â€˜bfloat16â€™:â€˜bincountâ€™:â€˜bitwise_notâ€™:â€˜bitwise_not_â€™:â€˜bmmâ€™:â€˜boolâ€™:â€˜byteâ€™:â€˜cauchy_â€™:â€˜ceilâ€™:â€˜ceil_â€™:â€˜charâ€™:â€˜choleskyâ€™:â€˜cholesky_inverseâ€™:â€˜cholesky_solveâ€™:â€˜chunkâ€™:â€˜clampâ€™:â€˜clamp_â€™:â€˜coalesceâ€™:â€˜contiguousâ€™:â€˜copy_â€™:â€˜cosâ€™:â€˜cos_â€™:â€˜coshâ€™:â€˜cosh_â€™:â€˜cpuâ€™:â€˜crossâ€™:â€˜cudaâ€™:â€˜cumprodâ€™:â€˜cumsumâ€™:â€˜dataâ€™:â€˜data_ptrâ€™:â€˜dense_dimâ€™:â€˜dequantizeâ€™:â€˜detâ€™:â€˜detachâ€™:â€˜detach_â€™:â€˜deviceâ€™:â€˜diagâ€™:â€˜diag_embedâ€™:â€˜diagflatâ€™:â€˜diagonalâ€™:â€˜digammaâ€™:â€˜digamma_â€™:â€˜dimâ€™:â€˜distâ€™:â€˜divâ€™:â€˜div_â€™:â€˜dotâ€™:â€˜doubleâ€™:â€˜dtypeâ€™:â€˜eigâ€™:â€˜element_sizeâ€™:â€˜eqâ€™:â€˜eq_â€™:â€˜equalâ€™:â€˜erfâ€™:â€˜erf_â€™:â€˜erfcâ€™:â€˜erfc_â€™:â€˜erfinvâ€™:â€˜erfinv_â€™:â€˜expâ€™:â€˜exp_â€™:â€˜expandâ€™:â€˜expand_asâ€™:â€˜expm1â€™:â€˜expm1â€™:â€˜exponentialâ€™:â€˜fftâ€™:â€˜fill_â€™:â€˜fill_diagonal_â€™:â€˜flattenâ€™:â€˜flipâ€™:â€˜floatâ€™:â€˜floorâ€™:â€˜floor_â€™:â€˜fmodâ€™:â€˜fmod_â€™:â€˜fracâ€™:â€˜frac_â€™:â€˜gatherâ€™:â€˜geâ€™:â€˜ge_â€™:â€˜geometric_â€™:â€˜geqrfâ€™:â€˜gerâ€™:â€˜get_deviceâ€™:â€˜grad_fnâ€™:â€˜gtâ€™:â€˜gt_â€™:â€˜halfâ€™:â€˜hardshrinkâ€™:â€˜has_namesâ€™:â€˜histcâ€™:â€˜ifftâ€™:â€˜index_addâ€™:â€˜index_add_â€™:â€˜index_copyâ€™:â€˜index_copy_â€™:â€˜index_fillâ€™:â€˜index_fill_â€™:â€˜index_putâ€™:â€˜index_put_â€™:â€˜index_selectâ€™:â€˜indicesâ€™:â€˜intâ€™:â€˜int_reprâ€™:â€˜inverseâ€™:â€˜irfftâ€™:â€˜is_coalescedâ€™:â€˜is_complexâ€™:â€˜is_contiguousâ€™:â€˜is_cudaâ€™:â€˜is_distributedâ€™:â€˜is_floating_pointâ€™:â€˜is_leafâ€™:â€˜is_mkldnnâ€™:â€˜is_nonzeroâ€™:â€˜is_pinnedâ€™:â€˜is_quantizedâ€™:â€˜is_same_sizeâ€™:â€˜is_set_toâ€™:â€˜is_sharedâ€™:â€˜is_signedâ€™:â€˜is_sparseâ€™:â€˜iscloseâ€™:â€˜itemâ€™:â€˜kthvalueâ€™:â€˜layoutâ€™:â€˜leâ€™:â€˜le_â€™:â€˜lerpâ€™:â€˜lerp_â€™:â€˜lgammaâ€™:â€˜lgamma_â€™:â€˜logâ€™:â€˜log10â€™:â€˜log10â€™:â€˜log1pâ€™:â€˜log1pâ€™:â€˜log2â€™:â€˜log2â€™:â€˜logâ€™:â€˜log_normal_â€™:â€˜log_softmaxâ€™:â€˜logdetâ€™:â€˜logical_notâ€™:â€˜logical_not_â€™:â€˜logical_xorâ€™:â€˜logical_xor_â€™:â€˜logsumexpâ€™:â€˜longâ€™:â€˜lstsqâ€™:â€˜ltâ€™:â€˜lt_â€™:â€˜luâ€™:â€˜lu_solveâ€™:â€˜map2â€™:â€˜mapâ€™:â€˜masked_fillâ€™:â€˜masked_fill_â€™:â€˜masked_scatterâ€™:â€˜masked_scatter_â€™:â€˜masked_selectâ€™:â€˜matmulâ€™:â€˜matrix_powerâ€™:â€˜maxâ€™:â€˜meanâ€™:â€˜medianâ€™:â€˜minâ€™:â€˜mmâ€™:â€˜modeâ€™:â€˜mulâ€™:â€˜mul_â€™:â€˜multinomialâ€™:â€˜mvâ€™:â€˜mvlgammaâ€™:â€˜mvlgamma_â€™:â€˜nameâ€™:â€˜namesâ€™:â€˜narrowâ€™:â€˜narrow_copyâ€™:â€˜ndimâ€™:â€˜ndimensionâ€™:â€˜neâ€™:â€˜ne_â€™:â€˜negâ€™:â€˜neg_â€™:â€˜nelementâ€™:â€˜newâ€™:â€˜new_emptyâ€™:â€˜new_fullâ€™:â€˜new_onesâ€™:â€˜new_tensorâ€™:â€˜new_zerosâ€™:â€˜nonzeroâ€™:â€˜normâ€™:â€˜normal_â€™:â€˜numelâ€™:â€˜numpyâ€™:â€˜orgqrâ€™:â€˜ormqrâ€™:â€˜output_nrâ€™:â€˜permuteâ€™:â€˜pin_memoryâ€™:â€˜pinverseâ€™:â€˜polygammaâ€™:â€˜polygamma_â€™:â€˜preluâ€™:â€˜prodâ€™:â€˜put_â€™:â€˜q_per_channel_axisâ€™:â€˜q_per_channel_scalesâ€™:â€˜q_per_channel_zero_pointsâ€™:â€˜q_scaleâ€™:â€˜q_zero_pointâ€™:â€˜qrâ€™:â€˜qschemeâ€™:â€˜random_â€™:â€˜reciprocalâ€™:â€˜reciprocal_â€™:â€˜record_streamâ€™:â€˜refine_namesâ€™:â€˜register_hookâ€™:â€˜reinforceâ€™:â€˜remainderâ€™:â€˜remainder_â€™:â€˜renameâ€™:â€˜rename_â€™:â€˜renormâ€™:â€˜renorm_â€™:â€˜repeatâ€™:â€˜repeat_interleaveâ€™:â€˜requires_gradâ€™:â€˜requires_grad_â€™:â€˜reshapeâ€™:â€˜reshape_asâ€™:â€˜resizeâ€™:â€˜resize_â€™:â€˜resize_asâ€™:â€˜resize_as_â€™:â€˜retain_gradâ€™:â€˜rfftâ€™:â€˜rollâ€™:â€˜rot90â€™:â€˜roundâ€™:â€˜round_â€™:â€˜rsqrtâ€™:â€˜rsqrt_â€™:â€˜scatterâ€™:â€˜scatter_â€™:â€˜scatter_addâ€™:â€˜scatter_add_â€™:â€˜selectâ€™:â€˜set_â€™:â€˜share_memory_â€™:â€˜shortâ€™:â€˜sigmoidâ€™:â€˜sigmoid_â€™:â€˜signâ€™:â€˜sign_â€™:â€˜sinâ€™:â€˜sin_â€™:â€˜sinhâ€™:â€˜sinh_â€™:â€˜sizeâ€™:â€˜slogdetâ€™:â€˜smmâ€™:â€˜softmaxâ€™:â€˜solveâ€™:â€˜sortâ€™:â€˜sparse_dimâ€™:â€˜sparse_maskâ€™:â€˜sparse_resize_â€™:â€˜sparse_resize_and_clear_â€™:â€˜splitâ€™:â€˜split_with_sizesâ€™:â€˜squeezeâ€™:â€˜squeeze_â€™:â€˜sspaddmmâ€™:â€˜stdâ€™:â€˜stftâ€™:â€˜storageâ€™:â€˜storage_offsetâ€™:â€˜storage_typeâ€™:â€˜subâ€™:â€˜sub_â€™:â€˜sumâ€™:â€˜sum_to_sizeâ€™:â€˜svdâ€™:â€˜symeigâ€™:â€˜takeâ€™:â€˜tanâ€™:â€˜tan_â€™:â€˜tanhâ€™:â€˜tanh_â€™:â€˜toâ€™:â€˜to_denseâ€™:â€˜to_mkldnnâ€™:â€˜to_sparseâ€™:â€˜tolistâ€™:â€˜topkâ€™:â€˜traceâ€™:â€˜transposeâ€™:â€˜transpose_â€™:â€˜triangular_solveâ€™:â€˜trilâ€™:â€˜tril_â€™:â€˜triuâ€™:â€˜triu_â€™:â€˜truncâ€™:â€˜trunc_â€™:â€˜typeâ€™:â€˜type_asâ€™:â€˜unbindâ€™:â€˜unflattenâ€™:â€˜unfoldâ€™:â€˜uniform_â€™:â€˜uniqueâ€™:â€˜unique_consecutiveâ€™:â€˜valuesâ€™:â€˜varâ€™:â€˜viewâ€™:â€˜view_asâ€™:â€˜whereâ€™:â€˜zero_â€™ "
    }, {
    "id": 16,
    "url": "http://localhost:4000/2019/11/note08-fastai-4/",
    "title": "Gradient backward",
    "body": "2019/11/26 - â€ Lecture 08 - Deep Learning From Foundations-part2 â€œ Homework:  calculus for machine learning     based on weightâ€¦    einsum conventionCONTENTS:  2. Foundation version     2. 3 Gradients backward pass         decompose function     chain rule with code     check the result using Pytorch autograd           3. Refactor model     3. 1 Layers as classes   3. 2 Modue. forward()   3. 3 Without einsum   3. 4 nn. Linear and nn. Module    Forward process 2. Foundation version: 2. 3 Gradients backward pass:  Gradients is output with respect to parameter weâ€™ve done this work in this path(below)  to simplify this calculus, we can just change it into,  So, you should know of the derivative of each bit on its own, and then you multiply them all together. As a result, it would be over cross over the data.  So you can get gradient, output with respect to parameter  What order should we calculate? BTW, why Jeremy wrote , not Loss function?1 decompose function We want to get derivative of which forms  But, we have a estimation of answer (we call it y hat) now So, I will decompose funciton to trace target variable.  Using the above forward pass, we can suppose some function from the end.  start from , We know MSE funciton got two parameters, output, and target .  from MSEâ€™s input we know functionâ€™s output and supposing v is input of that function,  similarly, v became output of chain rule with code   examplify backward process by random sampling     To get a variable, I modified forward model a little  def model_ping(out = 'x_train'):  l1 = lin(x_train, w1, b1) # one linear layer  l2 = relu(l1) # one relu layer  l3 = lin(l2, w2, b2) # one more linear layer  return eval(out) Be careful we donâ€™t use mse_loss in backward process1) start with the very last function, which is loss funciton. MSE  If we codify this formula,def mse_grad(inp, targ): #mse_input(1000,1), mse_targ (1000,1)  # grad of loss with respect to output of previous layer  inp. g = 2. * (inp. squeeze() - targ). unsqueeze(-1) / inp. shape[0] And, this can be examplified like below.  Notice that input of gradient function is same with forward functiony_hat = model_ping('l3') #get value from forward modely_hat. g = ((y_hat. squeeze(-1)-y_train). unsqueeze(-1))/y_hat. shape[0]y_hat. g. shape&gt;&gt;&gt; torch. Size([50000, 1]) We can just calculate using broadcasting, not using squeeze. then why should do and unsqueeze again?ğŸ¯ Itâ€™s related with random access memory(RAM). . If I donâ€™t squeeze, (Iâ€™m using colab) it out of RAM. 2) Derivative of linear2 function  This processâ€™s weight dimensions defined by axis=1, axis=2.  axis=0 dimension means size of data. This will be summazed by . sum(0) method.  unsqeeze(-1)&amp;unsqeeze(1) seperates the dimension, and make a dot product, and vanish axis=0 dimension. def lin_grad(inp, out, w, b):  # grad of matmul with respect to input  inp. g = out. g @ w. t()  w. g = (inp. unsqueeze(-1) * out. g. unsqueeze(1)). sum(0)  b. g = out. g. sum(0) Examplified belowlin2 = model_ping('l2'); #get value from forward modellin2. g = y_hat. g@w2. t(); w2. g = (lin2. unsqueeze(-1) * y_hat. g. unsqueeze(1)). sum(0);b2. g = y_hat. g. sum(0);lin2. g. shape, w2. g. shape, b2. g. shape&gt;&gt;&gt; torch. Size([50000, 50])torch. Size([50, 1])torch. Size([1]) Notice going reverse order, weâ€™re passing in gradient backward3) derivative of ReLU  def relu_grad(inp, out):  # grad of relu with respect to input activations  inp. g = (inp&gt;0). float() * out. g Examplified belowlin1=model_ping('l1') #get value from forward modellin1. g = (lin1&gt;0). float() * lin2. g;lin1. g. shape&gt;&gt;&gt; torch. Size([50000, 50])4) Derivative of linear1  Same process with 2) but, this processâ€™s weight hasdef lin_grad(inp, out, w, b):  # grad of matmul with respect to input  inp. g = out. g @ w. t()  w. g = (inp. unsqueeze(-1) * out. g. unsqueeze(1)). sum(0)  b. g = out. g. sum(0) Examplified belowx_train. g = lin1. g @ w1. t(); w1. g = (x_train. unsqueeze(-1) * lin1. g. unsqueeze(1)). sum(0); b1. g = lin1. g. sum(0);x_train. g. shape, w1. g. shape, b1. g. shape&gt;&gt;&gt; torch. Size([50000, 784])torch. Size([784, 50])torch. Size([50])5) Then it goes backward pass def forward_and_backward(inp, targ):  # forward pass:  l1 = inp @ w1 + b1  l2 = relu(l1)  out = l2 @ w2 + b2  # we don't actually need the loss in backward!  loss = mse(out, targ)    # backward pass:  mse_grad(out, targ)  lin_grad(l2, out, w2, b2)  relu_grad(l1, l2)  lin_grad(inp, l1, w1, b1)Version 1 (Basic)- Wall time: 1. 95 s Summary  Notice that output of function at forward pass became input of backward pass backpropagation is just the chain rule value loss (loss=mse(out,targ)) is not used in gradient calcuation.      Because, it doesnâ€™t appear with the weight.     w1g, w2g, b1g, b2g, ig will be used for optimizercheck the result using Pytorch autograd require_grad_ is the magical function, which can automatic differentiation. 2     This magical auto gradified tensor keep track what happend in forward (taking loss function),   and do the backward3   So it saves our time to differentiate ourselves   â¤µï¸ THis is benchmarkâ€¦. . Version 2 (torch autograd)- Wall time: 3. 81 Âµs 3. Refactor model:  Amazingly, just refactoring our main pieces, it comes down up to Pytorch package. ğŸŒŸ Implement yourself, Practice, practice, practice! ğŸŒŸ 3. 1 Layers as classes:    Relu and Linear are layers in oue neural net. -&gt; make it as classes     For the forward, using __call__ for the both of forward &amp; backward. Because â€˜callâ€™ means we treat this as a function.  class Lin():  def __init__(self, w, b): self. w,self. b = w,b      def __call__(self, inp):    self. inp = inp    self. out = inp@self. w + self. b    return self. out    def backward(self):    self. inp. g = self. out. g @ self. w. t()    # Creating a giant outer product, just to sum it, is inefficient!    self. w. g = (self. inp. unsqueeze(-1) * self. out. g. unsqueeze(1)). sum(0)    self. b. g = self. out. g. sum(0) Remember that in lin_grad function, we save bias&amp;weight!!!!!ğŸ’¬ inp. g : gradient of the output with respect to the input. {: style=â€color:grey; font-size: 90%; text-align: center;â€} ğŸ’¬ w. g : gradient of the output with respect to the weight. {: style=â€color:grey; font-size: 90%; text-align: center;â€} ğŸ’¬ b. g : gradient of the output with respect to the bias. {: style=â€color:grey; font-size: 90%; text-align: center;â€} class Model():  def __init__(self, w1, b1, w2, b2):    self. layers = [Lin(w1,b1), Relu(), Lin(w2,b2)]    self. loss = Mse()      def __call__(self, x, targ):    for l in self. layers: x = l(x)    return self. loss(x, targ)    def backward(self):    self. loss. backward()    for l in reversed(self. layers): l. backward()   refer to Jeremyâ€™s Model class, he put layers in list   Dionneâ€™s self-study note: Decomposing Jeremyâ€™s Model class     init needs weight, bias but not x data   when call that class(a. k. a function) it gave x data and y label!   jeremy composited function in layers. x = l(x) so conciseâ€¦. .    also utilized that layer list when backward ust reversing it (using python listâ€™s method)    And he is recursively calling the function on the result of the previous thing. â¬‡ï¸for l in self. layers:  x = l(x)Q2: Donâ€™t I need to declare magical autograd function, requires_grad_?{: style=â€color:red; font-size: 130%; text-align: center;â€} [The questions migrated to this article] Version 3 (refactoring - layer to class)- Wall time: 5. 25 Âµs 3. 2 Modue. forward():  Duplicate code makes execution time slow.      Role of __call__ changed. No more __call__ for implementing forward pass.    By initializing the forward with __call__, Module. forward() use overriding to maximize reusability. So any layer inherit Module, can use parentâ€™s function.     gradient of the output with respect to the weight  (self. inp. unsqueeze(-1) * self. out. g. unsqueeze(1)). sum(0)    can be reexpressed using einsum,   torch. einsum( bi,bj-&gt;ij , inp, out. g)    Defining forward and Module enables Pytorch to out almost duplicatesVersion 4 (Module &amp; einsum)- Wall time: 4. 29 Âµs Q2: Isnâ€™t there any way to use broadcasting? Why we should use outer product?{: style=â€color:red; font-size: 130%; text-align: center;â€} 3. 3 Without einsum: Replacing einsum to matrix product is even more faster. torch. einsum( bi,bj-&gt;ij , inp, out. g)can be reexpressed using matrix product, inp. t() @ out. gVersion 5 (without einsum)- Wall time: 3. 81 Âµs 3. 4 nn. Linear and nn. Module: Torchâ€™s package nn. Linear and nn. Module Version 6 (torch package)- Wall time: 5. 01 Âµs  Final, Using torch. nn. Linear &amp; torch. nn. Module~~~pythonclass Model(nn. Module):  def init(self, n_in, nh, n_out):    super(). init()    self. layers = [nn. Linear(n_in,nh), nn. ReLU(), nn. Linear(nh,n_out)]    self. loss = mse def __call__(self, x, targ):  for l in self. layers: x = l(x)  return self. loss(x. squeeze(), targ)class Model():  def init(self):    self. layers = [Lin(w1,b1), Relu(), Lin(w2,b2)]    self. loss = Mse() def __call__(self, x, targ):  for l in self. layers: x = l(x)  return self. loss(x, targ)def backward(self):  self. loss. backward()  for l in reversed(self. layers): l. backward()    ~~~ Footnote:       fast. ai forums Lesson-8Â &#8617;        pytorch docs - autogradÂ &#8617;        stackoverflow - finding methods a object hasÂ &#8617;    "
    }, {
    "id": 17,
    "url": "http://localhost:4000/2019/11/note08-fastai-3/",
    "title": "ReLU and data init with normailized data",
    "body": "2019/11/23 - â€ Lecture 08 - Deep Learning From Foundations-part2 â€œ Homework:  Review concepts 16 concepts from Course 1 (lessons 1 - 7)(1) Affine Functions &amp; non-linearities; 2) Parameters &amp; activations; 3) Random initialization &amp; transfer learning; 4) SGD, Momentum, Adam; 5) Convolutions; Batch-norm; 6) Dropout; 7) Data augmentation; 8) Weight decay; 9) Res/dense blocks; 10) Image classification and regression; 11)Embeddings; 12) Continuous &amp; Categorical variables; 13) Collaborative filtering; 14) Language models; 15) NLP classification; 16) Segmentation; U-net; GANS)  Make sure you understand broadcasting Read section 2. 2 in Delving Deep into Rectifiers Try to replicate as much of the notebooks as you can without peeking; when you get stuck, peek at the lesson notebook, but then close it and try to do it yourselfOther materials:  Understanding the difficulty of training deep feedforward neural networks, paper that introduced Xavier initializationCONTENTS:  Other materials 1. The forward and backward passes     1. 1 Normalization   1. 2 Variable definition    2. Foundation Version     2. 1 Basic architecture         2. 1. 1 simplified kaiming init     2. 1. 2 Glorrot initialization     2. 1. 3 Kaiming initializating     2. 1. 4 Pytorch package          2. 2 Loss function: MSE    Footnote* Normailzation{:toc}1. The forward and backward passes: 1. 1 Normalization: train_mean,train_std = x_train. mean(),x_train. std()&gt;&gt;&gt; train_mean,train_std(tensor(0. 1304), tensor(0. 3073))Remember!  Dataset, which is x_train, mean and standard deviation is not 0&amp;1. But we need them to be which means we should substract means and divide data by std.  You should not standarlize validation set because training set and validation set should be aparted.  after normalize, mean is close to zero, and standard deviation is close to 1. 1. 2 Variable definition:  n,m: size of the training set c: the number of activations we need in our model2. Foundation Version: 2. 1 Basic architecture:  Our model has one hidden layer, output to have 10 activations, used in cross entropy.    But in process of building architecture, we will use mean square error, output to have 1 activations and lator change it to cross entropy   number of hidden unit; 50see below pic  We want to make w1&amp;w2 mean and std be 0&amp;1.      why initializating and make mean zero and std one is important?   paper highlighting importance of normalisation - training 10,000 layer network without regularisation1   2. 1. 1 simplified kaiming initQ: Why we did init, normalize with only validation data? Because we can not handle and get statistics from each value of x_valid?{: style=â€color:red; font-size: 130%; text-align: center;â€}  what about hidden(first) layer?w1 = torch. randn(m,nh)b1 = torch. zeros(nh)t = lin(x_valid, w1, b1) # hidden&gt;&gt;&gt; t. mean(), t. std()((tensor(2. 3191), tensor(27. 0303))In output(second) layer, w2 = torch. randn(nh,1)b2 = torch. zeros(1)t2 = lin(t, w2, b2) # output&gt;&gt;&gt; t2. mean(), t2. std()(tensor(-58. 2665), tensor(170. 9717))   which is terribly far from normalzed value.     But if we apply simplified kaiming init  w1 = torch. randn(m,nh)/math. sqrt(m); b1 = torch. zeros(nh)w2 = torch. randn(nh,1)/math. sqrt(nh); b2 = torch. zeros(1)t = lin(x_valid, w1, b1)t. mean(),t. std()&gt;&gt;&gt; (tensor(-0. 0516), tensor(0. 9354)) But, actually, we use activations not only linear function After applying activations relu at linear layer, mean and deviation became 0. 5. 2. 1. 2 Glorrot initializationPaper2: Understanding the difficulty of training deep feedforward neural networks  Gaussian(, bell shaped, normal distributions) is not trained very well.  How to initialize neural nets? with the size of layer , the number of filters .  But there is No acount for import of ReLU If we got 1000 layers, vanishing gradients problem emerges2. 1. 3 Kaiming initializatingPaper3: Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification  Kaiming He, explained here rectifier: rectified linear unit rectifier network: neural network with rectifier linear units  This is kaiming init, and why suddenly replace one to two on a top?     to avoid vanishing gradient(weights)   But it doesnâ€™t give very nice mean tough.    2. 1. 4 Pytorch package Why fan_out?     according to pytorch documentation,   choosing 'fan_in' preserves the magnitude of the variance of the wights in the forward pass. choosing 'fan_out' preserves the magnitues in the backward pass(, which means matmul; with transposed matrix) â¡ï¸ in the other words, torch use fan_out cz pytorch transpose in linear transformaton.  What about CNN in Pytorch?I tried torch. nn. Conv2d. conv2d_forward?? Jeremy digged into using torch. nn. modules. conv. _ConvNd. reset_parameters?? 2  in Pytorch, it doesnâ€™t seem to be implemented kaiming init in right formula. so we should use our own operation.  But actually, this has been discussed in Pytorch community before. 3 4 Jeremy said it enhanced variance also, so I sampled 100 times and counted better results.  To make sure the shape seems sensible. check with assert. (remember we will replace 1 to 10 in cross entropy)assert model(x_valid). shape==torch. Size([x_valid. shape[0],1])&gt;&gt;&gt; model(x_valid). shape(10000, 1) We have made Relu, init, linear, it seems we can forward pass code we need for basic architecture nh = 50def lin(x, w, b): return x@w + b;w1 = torch. randn(m,nh)*math. sqrt(2. /m ); b1 = torch. zeros(nh)w2 = torch. randn(nh,1); b2 = torch. zeros(1)def relu(x): return x. clamp_min(0. ) - 0. 5t1 = relu(lin(x_valid, w1, b1))def model(xb):  l1 = lin(xb, w1, b1)  l2 = relu(l1)  l3 = lin(l2, w2, b2)  return l32. 2 Loss function: MSE:  Mean squared error need unit vector, so we remove unit axis.   def mse(output, targ): return (output. squeeze(-1) - targ). pow(2). mean()    In python, in case you remove axis, you use â€˜squeezeâ€™, or add axis use â€˜unsqueezeâ€™ torch. squeeze where code commonly broken. so, when you use squeeze, clarify dimension axis you want to removetmp = torch. tensor([1,1])tmp. squeeze()&gt;&gt;&gt; tensor([1, 1]) make sure to make as float when you calculateBut why??? because it is tensor?{: style=â€color:red; font-size: 130%;â€} Hereâ€™s the error when I donâ€™t transform the data type ---------------------------------------------------------------------------TypeError                 Traceback (most recent call last)&lt;ipython-input-22-ae6009bef8b4&gt; in &lt;module&gt;()----&gt; 1 y_train = get_data()[1] # call data again   2 mse(preds, y_train)TypeError: 'map' object is not subscriptable This is forward passFootnote:       Fixup Initialization: Residual Learning Without NormalizationÂ &#8617;        Pytorch implementaion on Kaiming init of conv and linear layersÂ &#8617;        Pytorch kaiming init issueÂ &#8617;        Pytorch kaiming init explainedÂ &#8617;    "
    }, {
    "id": 18,
    "url": "http://localhost:4000/2019/11/note08-fastai-2/",
    "title": "Broadcasting, Einstein sum, Pytorch operator",
    "body": "2019/11/21 - â€ Lecture 08 - Deep Learning From Foundations-part2 â€œ Time comparison with pure Python:    Matmul with broadcasting&gt; 3194. 95 times faster     Einstein summation&gt; 16090. 91 times faster     Pytorchâ€™s operator&gt; 49166. 67 times faster  Homework:  Frobenius Norm Review Broadcasting Review (especially Rule)     Refer colab! (I totally confused with extension of arrays)    torch. allclose Review np. einsum ReviewCONTENTS:  1. Elementwise op     1. 1 Frobenius norm    2. Elementwise Matmul 3. Broadcasting     2. 2 Matmul with broadcasting   2. 3 Broadcasting Rules    3. Einstein summation 4. Pytorch op But before that, we need initialized parameters and ReLU,     Footnote   1. Elementwise op: 1. 1 Frobenius norm:   above converted into (m*m). sum(). sqrt() Plus, donâ€™t suffer from mathmatical symbols. He also copy and paste that equations from wikipedia.  and if you need latex form, download it from archive. 2. Elementwise Matmul:  What is the meaning of elementwise?   We do not calculate each component. But all of the component at once. Because, length of column of A and row of B are fixed.   How much time we saved?  So now that takes 1. 37ms. We have removed one line of code and it is a 178 times fasterâ€¦#TODOI donâ€™t know where the 5 from. but keep it. Maybe this is related with frobenius normâ€¦?as a result, the code before for k in range(ac):  c[i,j] += a[i,k] + b[k,j]the code after c[i,j] = (a[i,:] * b[:,j]). sum()To compare it (result betweet original and adjusted version) we use not test_eq but other function. The reason for this is that due to rounding errors from math operations, matrices may not be exactly the same. As a result, we want a function that will â€œis a equal to b within some toleranceâ€ #exportdef near(a,b):   return torch. allclose(a, b, rtol=1e-3, atol=1e-5)def test_near(a,b):   test(a,b,near)test_near(t1, matmul(m1, m2))3. Broadcasting:  Now, we will use the broadcasting and removec[i,j] = (a[i,:] * b[:,j]). sum() How it works?&gt;&gt;&gt; a=tensor([[10,10,10],     [20,20,20],     [30,30,30]])&gt;&gt;&gt; b=tensor([1,2,3,])&gt;&gt;&gt; a,b  (tensor([[10, 10, 10],     [20, 20, 20],     [30, 30, 30]]),tensor([1, 2, 3]))     &gt;&gt;&gt; a+btensor([[11, 12, 13],    [21, 22, 23],    [31, 32, 33]])  &lt;Figure 2&gt; demonstrated how array b is broadcasting(or copied but not occupy memory) to compatible with a. Refered from numpy_tutorial   there is no loop, but it seems there is exactly the loop.     This is not from jeremy (actually after a moment he cover it) but i wondered How to broadcast an array by columns?  c=tensor([[1],[2],[3]])a+ctensor([[11, 11, 11],    [22, 22, 22],    [33, 33, 33]])s  What is tensor. stride()?help(t. stride)Help on built-in function stride:  stride(â€¦) method of torch. Tensor instancestride(dim) -&gt; tuple or intReturns the stride of :attr:â€™selfâ€™ tensor. Stride is the jump necessary to go from one element to the next one in the specified dimension :attr:â€™dimâ€™. A tuple of all strides is returned when no argument is passed in. Otherwise, an integer value is returned as the stride in the particular dimension :attr:â€™dimâ€™. Args:  dim (int, optional): the desired dimension in which stride is requiredExample::* x = torch. tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])``x. stride()` (5, 1)`x. stride(0)` 5`x. stride(-1)` 1   unsqueeze &amp; None index   We can manipulate rank of tensor Special value â€˜Noneâ€™, which means please squeeze a new axis here== please broadcast herec = torch. tensor([10,20,30])c[None,:] in c, squeeze a new axis in here please. 2. 2 Matmul with broadcasting: for i in range(ar):#  c[i,j] = (a[i,:]).      *[:,j]. sum() #previous  c[i]  = (a[i]. unsqueeze(-1) * b). sum(dim=0) And Using None also (As howard teached)c[i]  = (a[i ]. unsqueeze(-1) * b). sum(dim=0) #howardc[i]  = (a[i][:,None] * b). sum(dim=0) # using Nonec[i]  = (a[i,:,None]*b). sum(dim=0)â­ï¸TipsğŸŒŸ 1) Anytime thereâ€™s a trailinng(final) colon in numpy or pytorch you can delete it ex) c[i, :] = c [i]2) any number of colon commas at the start, you can switch it with the single elipsis.  ex) c[:,:,:,:,i] = c [â€¦,i] 2. 3 Broadcasting Rules:  What if we tensor. size([1,3]) * tensor. size([3,1])?  torch. Size([3, 3])    What is scale????   What if they are one array is times of the other array? ex) Image : 256 x 256 x 3Scale : 128 x 256 x 3Result: ?   Why I did not inserted axis via None, but happened broadcasting? &gt;&gt;&gt; c * c[:,None]tensor([[100. , 200. , 300. ],    [200. , 400. , 600. ],    [300. , 600. , 900. ]])maybe it broadcast cz following array has 3 rows as same principle, no matter what nature shape was, if we do the operation tensor broadcasts to the other. &gt;&gt;&gt; c==c[None]tensor([[True, True, True]])&gt;&gt;&gt; c[None]==c[None,:]tensor([[True, True, True]])&gt;&gt;&gt;c[None,:]==ctensor([[True, True, True]])3. Einstein summation:  Creates batch-wise, remove inner most loop, and replaced it with an elementwise producta. k. ac[i,j] += a[i,k] * b[k,j]inner most loop c[i,j] = (a[i,:] * b[:,j]). sum()elementwise product  Because K is repeated so we do a dot product. And it is torch. Usage of einsum()1) transpose2) diagnalisation tracing3) batch-wise (matmul) â€¦  einstein summation notationdef matmul(a,b): return torch. einsum('ik,kj-&gt;ij', a, b)so after all, we are now 16000 times faster than Python. 4. Pytorch op: 49166. 67 times faster than pure python And we will use this matrix multiplication in Fully Connect forward, with some initialized parameters and ReLU. But before that, we need initialized parameters and ReLU,: Footnote:  TensorRank ti note"
    }, {
    "id": 19,
    "url": "http://localhost:4000/2019/11/questions/",
    "title": "Questions studying Lesson 08",
    "body": "2019/11/20 - fast. ai v3 course: Normalization related to train and valid math:  The Matrix Calculus You Need For Deep Learningdata privacy:  SHA 256 metaclass Can I segment document without punctuation?python:    Broadcasting Review (especially Rule), (I totally confused with extension of arrays)1   method, attribute, thing,â€¦ for example pytorch tensor dir(var)     callable(getattr(loss, dir(loss))) -&gt; some gets false and other gets true      SyntaxError: positional argument follows keyword argument   I remember thatimport stringalphabet = string. ascii_lowercase[:26]alphabet. split()outputs splited by each character,,, but it doesnâ€™tâ€¦ why? machine resource, things:  Why RAM is expensive?      Refer colab!Â &#8617;    "
    }, {
    "id": 20,
    "url": "http://localhost:4000/2019/11/julia-evans/",
    "title": "Julia Evans",
    "body": "2019/11/20 - The women who surprised me in many ways. First, she approached me to teaching some concepts drawing cartoons. It was at Hackers news, which was hightest ranks. Personally I have the use of not to reading title, so and cartoon was so cute and clear. I naturally gonna understood mechanism and astonished by her explaination ability. Her value, which she was taught by many people so want to do same things, moved me. Volume of her knowledge, that just reading post title is a deal of work, amazed me. "
    }, {
    "id": 21,
    "url": "http://localhost:4000/2019/11/Git-Merge/",
    "title": "Why am I not listed as a contributor?!",
    "body": "2019/11/10 - From the end of last year, big changes have witnessed in NLP research. Embracing an unprecedented growth, I started to study new exciting results and advances. In doing so, I noticed Iâ€™m not listed as contributor of repo which my PR accessed. How did I come to a repository?: When Iâ€™m stuck, I would prefer to code, than to go deep in theory. (It must be so. . too much to understand ğŸ¤’)It was BERT released by Google AI I felt keenly the necessity of implementing, because not only couldnâ€™t understand the way they figured out positional encoding formula, but how it actually works. What does it mean to â€œscaleâ€ dot product in Attention? (Now I know itâ€™s far from my section ğŸ˜‚) Figure 1. Scaled Dot Product. Adopted from tensorflow blogWhat was the code error?: For implement code in paper, I read the papers Transformer and BERT, structured the model, and refered the othersâ€™ code. Meanwhile, I found out a small error in tokenization process, which was changing a token into [MASK], enabled bidirectional representation. Iâ€™ve made PR, and got merged. But I was not in contributors. Why?: Figure 2. Merged Pull request Adopted from graykode projectActually I happened to know there can be couple of reasons github doesnâ€™t include my name as contributor. Well, if contributors tab has more than 100 people, in which case it shows you up only if you are in the top 100 contributors because displaying too many contributors can make webpages down. Somethimes, however, it doesnâ€™t that problem. Why not? Two possibilities are there.    First, According to Joel-Glovier, if repository maintainer merged-as-a-rebase PR will end up showing as maintainerâ€™s commit. But maintainer shouldnâ€™t normally do this.     Second, if you happend to commit using a different git email that what is in your GitHub profile, it will not be attached to your Github user, and â€œdoesnâ€™t show upâ€ as you.  Reference:  MichaÅ‚ Chromiakâ€™s blog Github: why are my contributions are not showing on my profile atlassian-gitfetch"
    }, {
    "id": 22,
    "url": "http://localhost:4000/2019/11/elif-shafak/",
    "title": "Elif Shafak",
    "body": "2019/11/05 - For creative-minded people, Istanbul is a treasure. â€™ Photo Â© Chris Boland, licensed under CC BY-NC-ND 2. 0    it suddenly felt like what I was trying to convey was more complicated and detailed than what the circumstances allowed me to say.     And I did what I usually do in similar situations: I stammered, I shut down, and I stopped talking. I stopped talking because the truth was complicated, even though I knew, deep within, that one should never, ever remain silent for fear of complexity.         &lt;Figure 1&gt; Elif Shafak   Photo credit: www. elifsafak. com. tr      I want to talk about emotions and the need to boost our emotional intelligence. I think itâ€™s a pity that mainstream political theory pays very little attention to emotions.     Oftentimes, analysts and experts are so busy with data and metrics that they seem to forget those things in life that are difficult to measure and perhaps impossible to cluster under statistical models. But I think this is a mistake, for two main reasons. We are emotional beings.     I think itâ€™s going to be one of our biggest intellectual challenges, because our political systems are replete with emotions. In country after country, we have seen illiberal politicians exploiting these emotions. And yet within the academia and among the intelligentsia, we are yet to take emotions seriously. I think we should.  1 2 Reference:       British Council WorldwideÂ &#8617;        Ted TalkÂ &#8617;    "
    }, {
    "id": 23,
    "url": "http://localhost:4000/2019/11/coc-retropective/",
    "title": "Retrospective on Pycon 2019 Korea (CoC Committee)",
    "body": "2019/11/05 - When I was volunteer, it seems like busy and hectic to managing that crowded conference. In my experience, to get things moving, it needs hierarchy. But it didnâ€™t. Organizers emphasized our responsibility, and if I passed each otherâ€™s burden, It could be my burden next time. In solidarity of the obligation, we finished conference well. And after participating PyCon Korea 2018 as volunteer, Iâ€™ve joined PyCon Korea Organizer last year. &lt;Figure 1&gt; First meeting of PyCon 2019 Korea Organizers Itâ€™s been a while since PyCon 2019 finished. Itâ€™s held on Aug 15 - 18, at Coex Grand Balloom &lt;Figure 2&gt; Ongoing session, speaking on news comment processing &lt;Figure 3&gt; Sponsor Booth iin Coex Hall &lt;Figure 4&gt; After PyCon 2019, with all of volunteer, organizer, speakers ğŸ˜ ğŸ¥°  Serving as part of the coc TF, I spent large fraction of last year doing CoC job. hereâ€™s the path what weâ€™ve been grappled with to grasp a solution. First half: Before the conference Toward Diverse Community:  Formally weâ€™ve been reusing and modifying PyCon US CoC, but we needed fit in Korean and I was part of that to revise code of conduct. Except â€˜Thatâ€™ Diversity, Because it is â€˜Harassmentâ€™:  Specific point was harassment, and the others were not. process of finding the points. How can we settle this point?Second half: During the conference Handling the potential Harassment: Disjunction of policy and real-time situation: This â€˜PyCon 2019 Korea retrospective seriesâ€™ would be devided into 3 Episodes.  â€œRetrospective on Pycon 2019 Korea (CoC Committee)â€ â€œRetrospective on Pycon 2019 Korea (Program Chair)â€ (20 Nov, To Be Update) â€œMaintaining participation while still making timely decisionsâ€ (29 Nov, To Be Update)"
    }, {
    "id": 24,
    "url": "http://localhost:4000/2019/10/GPU-time/",
    "title": "5 reasons took much time to setting GPU for fast.ai than I expected",
    "body": "2019/10/23 - Motivation: Before now, me as a undergraduate student, I was parsimony who usually depend on colab, kaggle, friendâ€™s server(occasional) whenever i need GPU. . And this time itâ€™s been for a while to install GPU than I expected and I share the several component that stood in my way. Written at Oct 24 2019, if you think this is deprecated, please do not have a leap of faith. Just for the record, Iâ€™ve used Kaggle, Colab, GCP, Azure, EC2 as GPU cloud. 1. Did not know there is JupyterLab option in Google Cloud Platform. : At the first time when GCP came out, there was no AI Platform service. So from starting vm instance to launching jupyter and installing packages, I did all of the things myself. (and I learned ğŸ¤—) $	curl -O https://repo. continuum. io/archive/Anaconda3-5. 0. 1-Linux-x86_64. sh[Downloading conda in ssh] I created VM instance,selected zone, machine type and disk type. Then, define firewall rules and in ssh terminal, install jupyter and other packages. But you can do all of these things just using AI Platform.  [AI Platform] I think it especially save your time if you are living in Asia-Pacific, which google doesnâ€™t support not that much GPU resources.  2. Consider if the platform has limited resources in a region you live in. : I live in South Korea, East Asia, and it seems like this region has lots of limitation in GPU (except quite expensive AWS) And the Taiwan which was the only one region where I can launch my own VM with GPU (I tried all the other regions in the list) sometimes do normaly, but not always. ğŸ˜¥After launching, I did several works and next day I could not start VM. (I didnâ€™t count it, but tried it a few hours because I didnâ€™t want cost any more timeâ€¦) Endlessly failed to start instance, then I choose to move AWS as an alternative way. 3. Fast. ai gives deliberate guide and I didnâ€™t know it. : Fast. ai offer the guide for all available platform. (Colab, salamander, Gradient, Kaggle, Colab, and so on) It is so important, and really needs, because cloud computing options are vary as occasion and purpose arise. I didnâ€™t know fast. ai has manual to running GCP, and I think itâ€™s as good a reason as any for me to be have taken time. It helped me so much when I had aws and shortened my time. I donâ€™t want to read all of the manual in amazno. . (It is recommended. . but Iâ€™d rather read GIT PRO nowâ€¦) ssh -i ~/. ssh/&lt;your_private_key_pair&gt; -L localhost:8888:localhost:8888 ubuntu@&lt;your instance IP&gt;4. You should wait to add more volume just after add volume, by building AWS EC2. : Since Elastic Block Store(EBS) storage supports optimized storage, users canâ€™t extend storage volume two times in a row.  Unfortunately, at the first time, I didnâ€™t know it (again ğŸ‘») and when VM lacked volume, I doubled dist capacity (76*2) at a rough but It needs more.  &lt;!â€“ this time I installed GPU in two years, and it became little complicated compared to 2 years ago. And this time for the first time(maybe not the first time. . but i handled it in my class or with my friend. but itâ€™s my first time on my own. ) I very Iâ€™m started to using used google colab, kaggleand, GCP-JupyterLab, ec2 - friend made, aws vm machine but I had a environment variable but i did not know of it. On these days, I could not get a resources from taiwanâ€¦    I couldnâ€™t notice a deliberate     Anyway, as a result I tried myself gcp myself and aws ec2 with fast. ai But I think doing on my self surely takes much time (in this point I wonder why Iâ€™m doing this, and should remind me, especially I was studying disk volume optimization)     disk volume exceed - https://askubuntu. com/questions/919748/no-space-left-on-device-even-though-there-is:  "
    }, {
    "id": 25,
    "url": "http://localhost:4000/2019/10/note08-fastai-1/",
    "title": "Lecture 08 - Deep Learning From Foundations-part1",
    "body": "2019/10/18 - â€ Lecture 08 - Deep Learning From Foundations-part2 â€œ I donâ€™t know if you read this article, but I heartily appreciate Rachael Thomas and Jeremy Howard for providing these priceless lectures for free Homework: CONTENTS:  What is going on in this course?     What is â€˜from foundationsâ€™?   Steps to a basic modern CNN model   Todayâ€™s implementation goal: 1) matmul -&gt; 4) FC backward    Library development using jupyter notebook     jupyter notebook certainly can make module    Elementwise ops     How can we make python faster?         What is element wise operation?           ResourcesWhat is going on in this course?: What is â€˜from foundationsâ€™?: 1) Recreate fast. ai and Pytorch 2) using pure python  Evade OverfittingOverfit : validation error getting worsetraining loss &lt; validation loss  Know the name of the symbol you usefind in this page if you donâ€™t know the symbol that you are using or just draw it here (run by ML!) Steps to a basic modern CNN model:  1) Matrix multiplication -&gt; 2) Relu/Initialization -&gt; 3) Fully-connected Forward-&gt; 4) Fully-connected Backward -&gt; 5) Train loop -&gt; 6) Convolution-&gt; 7) Optimization -&gt;8) Batchnormalization -&gt; 9) Resnet Todayâ€™s implementation goal: 1) matmul -&gt; 4) FC backward: Library development using jupyter notebook: what is assers? jupyter notebook certainly can make module:  There will be #export tag that Howard (and we) want to extract special notebook2script. py will detect sign of #expert and convert following into python module and test ittest\_eq(TEST,'test')test\_eq(TEST,'test1')  what is run_notebook. py?     when you want to test your module in command line interface   		!python run\_notebook. py 01_matmul. ipynb  Is there any difference between 1) and 2)?1) test -&gt; test01 2) test01 -&gt; test #TODO I donâ€™t know yet  look into run_notebook. py, package fire Jeremy used. What is that?read and run the code in a notebook, and in the process, Jeremy made Python Fire library called!shockingly, fire takes any kind of function and converts into CLI command. fire library was released by Google open source, Thursday, March 2, 2017    Get data   pytorch and numpy are pretty much same.  variable c explains how many pixels there are in in MNIST, 28 pixels PyTorchâ€™s view() method: torch function that manipulating tensor, and squeeze() in torch &amp; mathmatical operation similar function Rao &amp; McMahan said usually this functions result in feature vector.    In part 1, you can use view function several times.     Initial python model     Which is Linear, like $Xw$(weight)$+a$(bias) $= Y$     If you donâ€™t know hou to multiple matrix, refer this site matmul visulization site   How many time spends if we we use pure python   function matmul, typical matrix multiplication function, takes about 1 second for calculating 1 single train data! (maybe assumed stochastic, 5 data points in validation)     it takes about 11. 36 hours to update parameters even single layer and 1 iteration! (if that was my computer, it would be 14 hours. . )ğŸ¤ª   THIS is why we need to consider â€˜timeâ€™&amp;â€™spaceâ€™ This is kinda slow - what if we could speed it up by 50,000 times? Letâ€™s try! Elementwise ops: How can we make python faster?:  If we want to calculate faster, then do remove pythonic calcuation, by passing its computation down to something that is written something other than python, like pytorch.  According to PyTorch doc it uses C++ (via ATen), so we are going to implement that function with python. What is element wise operation?:  items makes a pair, operate corresponding componentResources:  notebooks material video broadcasting excel"
    }, {
    "id": 26,
    "url": "http://localhost:4000/2019/09/code-first-course-note-16/",
    "title": "Lecture 16 - Code-First NLP Note",
    "body": "2019/09/03 - Algorithms can encode &amp; magnify human bias Case Study 1: Facial Recognition &amp; Predictive Policing:  Joy Buolamwini &amp; Timnit Gebru, gendershades. org     Microsoft, FACE+, IBM - All of these things are sell now.    Largest gap between $\therefore\ Lighter Male\ &gt;\ Darker\ Female $      This US mayor joked cops should â€œmount . 50-caliberâ€ guns where AI predicts crime      With machine learning, with automation, thereâ€™s a 99% success, so that robot is ã…¡will beã…¡99% accurate in telling us what is going to happen next, which is really interesting.     - city official in Lancater, CA, approving on using IBM for public security  Bias:  Bias is type of error Statistical Bias: difference between a statisticâ€™s expected value and the true value Unjust Bias: disproportionate preference for or prejudice against a group Unconscious bias: bias that we donâ€™t realize we have But, term bias is too generic to be productive. Different sources of bias have different causes Representation Bias: Dataset was not representative of the algorithm that might be used on later. Above : Data is okay, but algorithm has some problem. Below : Data has error.  For example, object detection production that performs very well in common product of US. But in contrast, change of target product region, like Zimbabwe, Solomon Island, and so on, reduced the performence remarkably. It is not the algorithmic problem, so we should care about data volume of region. Evaluation Bias: Benchmark datasets spur on research, 4. 4% of IJB-A images are dark-skinned women. 2/3 of ImageNet images from the West (Sharkar et al, 2017) Case Study 2: Recidivism Algorithm Used Prison Sentencing: Case Study 3: Online Ad Delivery: Bias in NLP: ( Nothing to do with the course, but Iâ€™m researching this field these days. )    But all about Englsih     ImpactThe person is doctor. The person is nurse -&gt; ê·¸ëŠ” ì˜ì‚¬ë‹¤. ê·¸ë…€ëŠ” ê°„í˜¸ì‚¬ë‹¤.  Concept of â€œbiased dataâ€ often too generic to be useful:  Different sources of bias have different sources Data, models and systems are not unchanging numbers on a screen. Theyâ€™re the result of a complex process that starts with years of historical context and involves a series of choices and norms, from data measurement to model evaluation to human interpretation. - Harini Suresh, â€œThe problem with Biased Dataâ€ Five Sources of Bias in ML:  Representation Bias Evaluation Bias Measurement Bias Aggregation Bias(46:02) Historical Bias(46:26)     A few studies(47:13)   Racial Bias, Even when we have good intentions(new york times)(47:10)   gender(48:59)   Humans are biased, so why does algorithmic bias matter?: Algorithms &amp; humans are used differently (humans are usually decision maker):  Algorithms are accurate and objective No way to apeal if there if error processed large scale cheap Machine learning can amplify bias: Machine learning can create feedback loops. : Technology is power. And with that comes responsibility. : Solutions:  Analyze a project at work/school:     Questions about AI   5 types of bias (Suresh &amp; Guttag)   Datasheets for datasets, Modelcards for model reporting   Accuracy rate on different sub-groups    Work with domain experts &amp; those impacted Increase diversity in our workspace Advocate for good policy Be on the ongoing lookout for bias"
    }];

var idx = lunr(function () {
    this.ref('id')
    this.field('title')
    this.field('body')

    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});


    
function lunr_search(term) {
    $('#lunrsearchresults').show( 1000 );
    $( "body" ).addClass( "modal-open" );
    
    document.getElementById('lunrsearchresults').innerHTML = '<div id="resultsmodal" class="modal fade show d-block"  tabindex="-1" role="dialog" aria-labelledby="resultsmodal"> <div class="modal-dialog shadow-lg" role="document"> <div class="modal-content"> <div class="modal-header" id="modtit"> <button type="button" class="close" id="btnx" data-dismiss="modal" aria-label="Close"> &times; </button> </div> <div class="modal-body"> <ul class="mb-0"> </ul>    </div> <div class="modal-footer"><button id="btnx" type="button" class="btn btn-secondary btn-sm" data-dismiss="modal">Close</button></div></div> </div></div>';
    if(term) {
        document.getElementById('modtit').innerHTML = "<h5 class='modal-title'>Search results for '" + term + "'</h5>" + document.getElementById('modtit').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><small><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></small></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>Sorry, no results found. Close & try a different search!</li>";
        }
    }
    return false;
}
</script>
<style>
    .lunrsearchresult .title {color: #d9230f;}
    .lunrsearchresult .url {color: silver;}
    .lunrsearchresult a {display: block; color: #777;}
    .lunrsearchresult a:hover, .lunrsearchresult a:focus {text-decoration: none;}
    .lunrsearchresult a:hover .title {text-decoration: underline;}
</style>




<form class="bd-search hidden-sm-down" onSubmit="return lunr_search(document.getElementById('lunrsearch').value);">
<input type="text" class="form-control text-small"  id="lunrsearch" name="q" value="" placeholder="Type keyword and enter..."> 
</form>
            </ul>
        </div>
    </div>
    </nav>

    <!-- Search Results -->
    <div id="lunrsearchresults">
        <ul class="mb-0"></ul>
    </div>

    <!-- Content -->
    <main role="main" class="site-content">
        <div class="container">
    

    
    
    
<!-- Begin post excerpts, let's highlight the first 4 posts on top -->
<div class="row remove-site-content-margin">
    
    <!-- latest post -->
    
    <div class="col-md-6">
    <div class="card border-0 mb-4 box-shadow">   
    <a href="/2019/12/convolution/">
    <div class="topfirstimage" style="background-image: url( /assets/images/cnn-jiwon.png); height: 200px;    background-size: cover;    background-repeat: no-repeat;"></div>     
    </a>
    <div class="card-body px-0 pb-0 d-flex flex-column align-items-start">
    <h2 class="h4 font-weight-bold">
    <a class="text-dark" href="/2019/12/convolution/">What is a convolution?</a>
    </h2>
    <p class="excerpt">
        This is the part of Journey which Jeremy recommended us to do.One of the concepts I have to know.
    </p>
    <div>
        <small class="d-block text-muted">
            In <span class="catlist">
                
                <a class="text-capitalize text-muted smoothscroll" href="/categories.html#fast.ai-v3">Fast.AI-v3</a><span class="sep">, </span>
                
                </span>                   
        </small>
        <small class="text-muted">
            Dec 16, 2019
        </small>
    </div>
    </div>
    </div>
    </div>
    
    <div class="col-md-6">
        
        <!-- second latest post --><div class="mb-3 d-flex align-items-center">                
                
                <div class="col-md-4">
                <a href="/2019/12/jeremy-howard/">
                 <img class="w-100" src="http://localhost:4000/assets/images/jeremy-howard.jpeg" alt="Jeremy Howard">
                </a>
                </div>
                                
                <div>
                    <h2 class="mb-2 h6 font-weight-bold">
                    <a class="text-dark" href="/2019/12/jeremy-howard/">Jeremy Howard</a>
                    </h2>
                    <small class="d-block text-muted">
                        In <span class="catlist">
                        
                        <a class="text-capitalize text-muted smoothscroll" href="/categories.html#people in the world">People in the World</a><span class="sep">, </span>
                        
                        </span>                   
                    </small>
                    <small class="text-muted">
                        Dec 15, 2019
                    </small>
                </div>
            </div>
        
        <!-- third latest post --><div class="mb-3 d-flex align-items-center">                
                
                <div class="col-md-4">
                <a href="/2019/12/note02-fastai-1/">
                 <img class="w-100" src="/munich.jpg" alt="Making my own classifier with my own data">
                </a>
                </div>
                                
                <div>
                    <h2 class="mb-2 h6 font-weight-bold">
                    <a class="text-dark" href="/2019/12/note02-fastai-1/">Making my own classifier with my own data</a>
                    </h2>
                    <small class="d-block text-muted">
                        In <span class="catlist">
                        
                        <a class="text-capitalize text-muted smoothscroll" href="/categories.html#fast.ai-v3">Fast.AI-v3</a><span class="sep">, </span>
                        
                        </span>                   
                    </small>
                    <small class="text-muted">
                        Dec 14, 2019
                    </small>
                </div>
            </div>
        
        <!-- fourth latest post --><div class="mb-3 d-flex align-items-center">                
                
                <div class="col-md-4">
                <a href="/2019/12/pytorch-tensor/">
                <img class="w-100" src="/assets/images/pytorch.png" alt="Pytorch">
                </a>
                </div>
                                
                <div>
                    <h2 class="mb-2 h6 font-weight-bold">
                    <a class="text-dark" href="/2019/12/pytorch-tensor/">Pytorch</a>
                    </h2>
                    <small class="d-block text-muted">
                        In <span class="catlist">
                        
                        <a class="text-capitalize text-muted smoothscroll" href="/categories.html#library">library</a><span class="sep">, </span>
                        
                        </span>                   
                    </small>
                    <small class="text-muted">
                        Dec 12, 2019
                    </small>
                </div>
            </div>
        
    </div>
    
</div>
    
<!-- Sticky - add sticky tag to the post you want to highlight here - tags: [sticky] -->
 

<div class="jumbotron jumbotron-fluid jumbotron-home pt-0 pb-0 mt-3 mb-2rem bg-lightblue position-relative">
    <div class="pl-4 pr-0 h-100 tofront">
        <div class="row justify-content-between">
            <div class="col-md-6 pt-6 pb-6 pr-lg-4 align-self-center">
                <h1 class="mb-3">What is a convolution?</h1>
                <p class="mb-3 lead">
                    This is the part of Journey which Jeremy recommended us to do.One of the concepts I have to know.
                </p>
                <a href="/2019/12/convolution/" class="btn btn-dark">Read More</a>
            </div>
            <div class="col-md-6 d-none d-md-block pr-0" style="background-size:cover;background-image:url(/assets/images/cnn-jiwon.png);">	
            </div>
        </div>
    </div>
</div> 

 

 

 

 

 

 

 

 

 

 

 

 

 

 




    


 <!--endif page url is / -->
    


<!-- Now the rest of the posts with the usual loop but with an offset:4 on the first page so we can skeep the first 4 posts displayed above -->
    
<div class="row mt-3">
   
    <div class="col-md-8 main-loop">
        
        <h4 class="font-weight-bold spanborder"><span>All Stories</span></h4>
        

        
        
            <div class="mb-5 d-flex justify-content-between main-loop-card">
<div class="pr-3">
	<h2 class="mb-1 h4 font-weight-bold">
	<a class="text-dark" href="/2019/12/convolution/">What is a convolution?</a>
	</h2>
	<p class="excerpt">
	   This is the part of Journey which Jeremy recommended us to do.One of the concepts I have to know.
	</p>
	<small class="d-block text-muted">
		In <span class="catlist">
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#fast.ai-v3">Fast.AI-v3</a><span class="sep">, </span>
		
		</span>
	</small>
	<small class="text-muted">
		Dec 16, 2019
	</small>
</div>

	<div class="col-md-3 pr-0 text-right">
	<a href="/2019/12/convolution/">
	<img class="w-100" src="/assets/images/cnn-jiwon.png" alt="What is a convolution?">
	</a>
	</div>

</div>
        
        
        
            <div class="mb-5 d-flex justify-content-between main-loop-card">
<div class="pr-3">
	<h2 class="mb-1 h4 font-weight-bold">
	<a class="text-dark" href="/2019/12/jeremy-howard/">Jeremy Howard</a>
	</h2>
	<p class="excerpt">
	   How he impacted me?
	</p>
	<small class="d-block text-muted">
		In <span class="catlist">
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#people in the world">People in the World</a><span class="sep">, </span>
		
		</span>
	</small>
	<small class="text-muted">
		Dec 15, 2019
	</small>
</div>

	<div class="col-md-3 pr-0 text-right">
	<a href="/2019/12/jeremy-howard/">
	<img class="w-100" src="/assets/images/jeremy-howard.jpeg" alt="Jeremy Howard">
	</a>
	</div>

</div>
        
        
        
            <div class="mb-5 d-flex justify-content-between main-loop-card">
<div class="pr-3">
	<h2 class="mb-1 h4 font-weight-bold">
	<a class="text-dark" href="/2019/12/note02-fastai-1/">Making my own classifier with my own data</a>
	</h2>
	<p class="excerpt">
	   Making a classifier which can seperate Munich from Berlin and Hamburg!(hoping my well in Munich!ğŸ¤Ÿ)
	</p>
	<small class="d-block text-muted">
		In <span class="catlist">
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#fast.ai-v3">Fast.AI-v3</a><span class="sep">, </span>
		
		</span>
	</small>
	<small class="text-muted">
		Dec 14, 2019
	</small>
</div>

	<div class="col-md-3 pr-0 text-right">
	<a href="/2019/12/note02-fastai-1/">
	<img class="w-100" src="/munich.jpg" alt="Making my own classifier with my own data">
	</a>
	</div>

</div>
        
        
        
            <div class="mb-5 d-flex justify-content-between main-loop-card">
<div class="pr-3">
	<h2 class="mb-1 h4 font-weight-bold">
	<a class="text-dark" href="/2019/12/pytorch-tensor/">Pytorch</a>
	</h2>
	<p class="excerpt">
	     This is the list when I type
	</p>
	<small class="d-block text-muted">
		In <span class="catlist">
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#library">library</a><span class="sep">, </span>
		
		</span>
	</small>
	<small class="text-muted">
		Dec 12, 2019
	</small>
</div>

	<div class="col-md-3 pr-0 text-right">
	<a href="/2019/12/pytorch-tensor/">
	<img class="w-100" src="/assets/images/pytorch.png" alt="Pytorch">
	</a>
	</div>

</div>
        
        
        
            <div class="mb-5 d-flex justify-content-between main-loop-card">
<div class="pr-3">
	<h2 class="mb-1 h4 font-weight-bold">
	<a class="text-dark" href="/2019/11/note08-fastai-4/">Gradient backward</a>
	</h2>
	<p class="excerpt">
	   â€ Lecture 08 - Deep Learning From Foundations-part2 â€œ
	</p>
	<small class="d-block text-muted">
		In <span class="catlist">
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#fast.ai-v3">Fast.AI-v3</a><span class="sep">, </span>
		
		</span>
	</small>
	<small class="text-muted">
		Nov 26, 2019
	</small>
</div>

	<div class="col-md-3 pr-0 text-right">
	<a href="/2019/11/note08-fastai-4/">
	<img class="w-100" src="/assets/images/4-backward.png" alt="Gradient backward">
	</a>
	</div>

</div>
        
        
        
            <div class="mb-5 d-flex justify-content-between main-loop-card">
<div class="pr-3">
	<h2 class="mb-1 h4 font-weight-bold">
	<a class="text-dark" href="/2019/11/note08-fastai-3/">ReLU and data init with normailized data</a>
	</h2>
	<p class="excerpt">
	   â€ Lecture 08 - Deep Learning From Foundations-part2 â€œ
	</p>
	<small class="d-block text-muted">
		In <span class="catlist">
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#fast.ai-v3">Fast.AI-v3</a><span class="sep">, </span>
		
		</span>
	</small>
	<small class="text-muted">
		Nov 23, 2019
	</small>
</div>

	<div class="col-md-3 pr-0 text-right">
	<a href="/2019/11/note08-fastai-3/">
	<img class="w-100" src="/assets/images/2-relu.png" alt="ReLU and data init with normailized data">
	</a>
	</div>

</div>
        
        
        
            <div class="mb-5 d-flex justify-content-between main-loop-card">
<div class="pr-3">
	<h2 class="mb-1 h4 font-weight-bold">
	<a class="text-dark" href="/2019/11/note08-fastai-2/">Broadcasting, Einstein sum, Pytorch operator</a>
	</h2>
	<p class="excerpt">
	   â€ Lecture 08 - Deep Learning From Foundations-part2 â€œ
	</p>
	<small class="d-block text-muted">
		In <span class="catlist">
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#fast.ai-v3">Fast.AI-v3</a><span class="sep">, </span>
		
		</span>
	</small>
	<small class="text-muted">
		Nov 21, 2019
	</small>
</div>

	<div class="col-md-3 pr-0 text-right">
	<a href="/2019/11/note08-fastai-2/">
	<img class="w-100" src="/assets/images/30.png" alt="Broadcasting, Einstein sum, Pytorch operator">
	</a>
	</div>

</div>
        
        
        
            <div class="mb-5 d-flex justify-content-between main-loop-card">
<div class="pr-3">
	<h2 class="mb-1 h4 font-weight-bold">
	<a class="text-dark" href="/2019/11/questions/">Questions studying Lesson 08</a>
	</h2>
	<p class="excerpt">
	   fast.ai v3 course
	</p>
	<small class="d-block text-muted">
		In <span class="catlist">
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#qna">QnA</a><span class="sep">, </span>
		
		</span>
	</small>
	<small class="text-muted">
		Nov 20, 2019
	</small>
</div>

</div>
        
        
        
            <div class="mb-5 d-flex justify-content-between main-loop-card">
<div class="pr-3">
	<h2 class="mb-1 h4 font-weight-bold">
	<a class="text-dark" href="/2019/11/julia-evans/">Julia Evans</a>
	</h2>
	<p class="excerpt">
	   The women who surprised me in many ways.First, she approached me to teaching some concepts drawing cartoons.It was at Hackers news, which was hightest ranks.Personally I have the use ...
	</p>
	<small class="d-block text-muted">
		In <span class="catlist">
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#people in the world">People in the World</a><span class="sep">, </span>
		
		</span>
	</small>
	<small class="text-muted">
		Nov 20, 2019
	</small>
</div>

	<div class="col-md-3 pr-0 text-right">
	<a href="/2019/11/julia-evans/">
	<img class="w-100" src="/assets/images/evans.jpg" alt="Julia Evans">
	</a>
	</div>

</div>
        
        
        
            <div class="mb-5 d-flex justify-content-between main-loop-card">
<div class="pr-3">
	<h2 class="mb-1 h4 font-weight-bold">
	<a class="text-dark" href="/2019/11/Git-Merge/">Why am I not listed as a contributor?!</a>
	</h2>
	<p class="excerpt">
	   From the end of last year, big changes have witnessed in NLP research.Embracing an unprecedented growth, I started to study new exciting results and advances.In doing so, I noticed Iâ€™...
	</p>
	<small class="d-block text-muted">
		In <span class="catlist">
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#resource">Resource</a><span class="sep">, </span>
		
		</span>
	</small>
	<small class="text-muted">
		Nov 10, 2019
	</small>
</div>

	<div class="col-md-3 pr-0 text-right">
	<a href="/2019/11/Git-Merge/">
	<img class="w-100" src="/assets/images/1.png" alt="Why am I not listed as a contributor?!">
	</a>
	</div>

</div>
        
        
        
        <div class="mt-5">
         <!-- Pagination links -->
            
            <ul class="pagination"> 
              
                <li class="page-item disabled"><span class="prev page-link">&laquo;</span></li>
              

              
                
                <li class="page-item disabled"><span class="webjeda page-link">1</span></li>
                
              
                
                <li class="page-item"><a class="page-link" href="/page2">2</a></li>
                
              

              
                <li class="page-item"><a class="page-link" href="/page2">Next &raquo;</a></li>
              
            </ul>
                  
        </div>
        
    </div>
    
    <div class="col-md-4">
        <div class="sticky-top sticky-top-offset">
    <h4 class="font-weight-bold spanborder"><span>Featured</span></h4>  
    <ol class="list-featured">				
                        
            <li class="mb-4">
            <span>
                <h6 class="font-weight-bold">
                    <a href="/2019/12/jeremy-howard/" class="text-dark">Jeremy Howard</a>
                </h6>
                <span class="d-block text-muted">
                    In <span class="catlist">
                    
                    <a class="text-capitalize text-muted smoothscroll" href="/categories.html#people in the world">People in the World</a><span class="sep">, </span>
                    
                    </span>
                </span>
            </span>
            </li>                
                        
            <li class="mb-4">
            <span>
                <h6 class="font-weight-bold">
                    <a href="/2019/12/note02-fastai-1/" class="text-dark">Making my own classifier with my own data</a>
                </h6>
                <span class="d-block text-muted">
                    In <span class="catlist">
                    
                    <a class="text-capitalize text-muted smoothscroll" href="/categories.html#fast.ai-v3">Fast.AI-v3</a><span class="sep">, </span>
                    
                    </span>
                </span>
            </span>
            </li>                
                        
            <li class="mb-4">
            <span>
                <h6 class="font-weight-bold">
                    <a href="/2019/12/pytorch-tensor/" class="text-dark">Pytorch</a>
                </h6>
                <span class="d-block text-muted">
                    In <span class="catlist">
                    
                    <a class="text-capitalize text-muted smoothscroll" href="/categories.html#library">library</a><span class="sep">, </span>
                    
                    </span>
                </span>
            </span>
            </li>                
                        
            <li class="mb-4">
            <span>
                <h6 class="font-weight-bold">
                    <a href="/2019/11/note08-fastai-4/" class="text-dark">Gradient backward</a>
                </h6>
                <span class="d-block text-muted">
                    In <span class="catlist">
                    
                    <a class="text-capitalize text-muted smoothscroll" href="/categories.html#fast.ai-v3">Fast.AI-v3</a><span class="sep">, </span>
                    
                    </span>
                </span>
            </span>
            </li>                
                        
            <li class="mb-4">
            <span>
                <h6 class="font-weight-bold">
                    <a href="/2019/11/note08-fastai-3/" class="text-dark">ReLU and data init with normailized data</a>
                </h6>
                <span class="d-block text-muted">
                    In <span class="catlist">
                    
                    <a class="text-capitalize text-muted smoothscroll" href="/categories.html#fast.ai-v3">Fast.AI-v3</a><span class="sep">, </span>
                    
                    </span>
                </span>
            </span>
            </li>                
                        
            <li class="mb-4">
            <span>
                <h6 class="font-weight-bold">
                    <a href="/2019/11/note08-fastai-2/" class="text-dark">Broadcasting, Einstein sum, Pytorch operator</a>
                </h6>
                <span class="d-block text-muted">
                    In <span class="catlist">
                    
                    <a class="text-capitalize text-muted smoothscroll" href="/categories.html#fast.ai-v3">Fast.AI-v3</a><span class="sep">, </span>
                    
                    </span>
                </span>
            </span>
            </li>                
                        
            <li class="mb-4">
            <span>
                <h6 class="font-weight-bold">
                    <a href="/2019/11/julia-evans/" class="text-dark">Julia Evans</a>
                </h6>
                <span class="d-block text-muted">
                    In <span class="catlist">
                    
                    <a class="text-capitalize text-muted smoothscroll" href="/categories.html#people in the world">People in the World</a><span class="sep">, </span>
                    
                    </span>
                </span>
            </span>
            </li>                
                        
            <li class="mb-4">
            <span>
                <h6 class="font-weight-bold">
                    <a href="/2019/11/Git-Merge/" class="text-dark">Why am I not listed as a contributor?!</a>
                </h6>
                <span class="d-block text-muted">
                    In <span class="catlist">
                    
                    <a class="text-capitalize text-muted smoothscroll" href="/categories.html#resource">Resource</a><span class="sep">, </span>
                    
                    </span>
                </span>
            </span>
            </li>                
                        
            <li class="mb-4">
            <span>
                <h6 class="font-weight-bold">
                    <a href="/2019/10/GPU-time/" class="text-dark">5 reasons took much time to setting GPU for fast.ai than I expected</a>
                </h6>
                <span class="d-block text-muted">
                    In <span class="catlist">
                    
                    <a class="text-capitalize text-muted smoothscroll" href="/categories.html#resource">Resource</a><span class="sep">, </span>
                    
                    </span>
                </span>
            </span>
            </li>                
                        
            <li class="mb-4">
            <span>
                <h6 class="font-weight-bold">
                    <a href="/2019/10/note08-fastai-1/" class="text-dark">Lecture 08 - Deep Learning From Foundations-part1</a>
                </h6>
                <span class="d-block text-muted">
                    In <span class="catlist">
                    
                    <a class="text-capitalize text-muted smoothscroll" href="/categories.html#fast.ai-v3">Fast.AI-v3</a><span class="sep">, </span>
                    
                    </span>
                </span>
            </span>
            </li>                
           
    </ol>
</div>     
    </div>
    
</div>



</div>
    </main>


    <!-- Scripts: popper, bootstrap, theme, lunr -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js" integrity="sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut" crossorigin="anonymous"></script>

    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js" integrity="sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k" crossorigin="anonymous"></script>

    <script src="/assets/js/theme.js"></script>


    <!-- Footer -->
    <footer class="bg-white border-top p-3 text-muted small">
        <div class="container">
        <div class="row align-items-center justify-content-between">
            <div>
                <span class="navbar-brand mr-2 mb-0"><strong>SpellOnYou</strong></span>
                <span>Copyright Â© <script>document.write(new Date().getFullYear())</script>.</span>

                <!--  Github Repo Star Btn-->
                <a class="text-dark ml-1" target="_blank" href="https://github.com/spellonyou"><i class="fab fa-github"></i> VisitMe</a>

            </div>
            <div>
                Made with <a target="_blank" class="text-dark font-weight-bold" href="https://www.wowthemes.net/mundana-jekyll-theme/"> Mundana Jekyll Theme </a> by <a class="text-dark" target="_blank" href="https://www.wowthemes.net">WowThemes</a>.
            </div>
        </div>
        </div>
    </footer>

    <!-- All this area goes before </body> closing tag --> 


</body>

</html>
